{
  
    
        "post0": {
            "title": "My introduction to Julia!",
            "content": "Hey! This post is about my introduction to the world of Julia. I took this challenge of learning Julia and making something in it. Since Julia is pretty similar to Python, I made a hypothesis. That is can I learn julia in a day and be up and running with something in two days? That was the challenge I took. Believe me or not but I did do it. Actually, if you&#39;re from a python background and have some expereince in it, then learning Julia is going to be fun and breezy for you. So, here I am after my two day rendezvous with Julia for two days. . So, what I used to learn Julia? I used resources from julia academy . What did I implement? I decided to go for one the resources I learnt deep learning from: Neural Networks and Deep Learning . Impemented the Julia version of Week 2 assignment of Neural Networks and Deep Learning course. . | It&#39;s not exactly that actually, I have applied the concepts to banknote authentication. . | Data is from UCI machine learning repository | Training is done, inference is on the way! | . I hope it&#39;s useful to you. It was a lot of fun and I am in love with Julia ‚ù§ . Let&#39;s begin! . using DelimitedFiles using Random using Plots . file = DelimitedFiles.readdlm(&quot;C: Users Abhishek Swain Desktop Julia_ML data_banknote_authentication.txt&quot;, &#39;,&#39;); . See the first 5 rows of the given array . function head(file) file[1:5, 1:5] end . head (generic function with 1 method) . head(file) . 5√ó5 Array{Float64,2}: 3.6216 8.6661 -2.8073 -0.44699 0.0 4.5459 8.1674 -2.4586 -1.4621 0.0 3.866 -2.6383 1.9242 0.10645 0.0 3.4566 9.5228 -4.0112 -3.5944 0.0 0.32924 -4.4552 4.5718 -0.9888 0.0 . Splitting into train &amp; test set . splits the data into train and test set . function train_test_split(file, at=0.7) n = size(file, 1) idx = shuffle(1:n) train_idx = view(idx, 1:floor(Int, at*n)) test_idx = view(idx, (floor(Int, at*n)+1):n) file = convert(Matrix, file) return file[train_idx,:], file[test_idx,:] end . train_test_split (generic function with 2 methods) . data_train, data_test = train_test_split(file, 0.25); X, y = data_train[:, 1:4], data_train[:, 5]; . Mathematical expression of the algorithm: . For one example $x^{(i)}$: $$z^{(i)} = w^T x^{(i)} + b tag{1}$$ $$ hat{y}^{(i)} = a^{(i)} = sigmoid(z^{(i)}) tag{2}$$ $$ mathcal{L}(a^{(i)}, y^{(i)}) = - y^{(i)} log(a^{(i)}) - (1-y^{(i)} ) log(1-a^{(i)}) tag{3}$$ . The cost is then computed by summing over all training examples: $$ J = frac{1}{m} sum_{i=1}^m mathcal{L}(a^{(i)}, y^{(i)}) tag{6}$$ . Sigmoid . Applies sigmoid to the vector . function œÉ(z) &quot;&quot;&quot; Compute the sigmoid of z &quot;&quot;&quot; return one(z) / (one(z) + exp(-z)) end . œÉ (generic function with 1 method) . Random initialization . Initialize w &amp; b with with random values between (0, 1) . function initialize(dim) &quot;&quot;&quot; This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0. Argument: dim -- size of the w vector we want (or number of parameters in this case) Returns: w -- initialized vector of shape (dim, 1) b -- initialized scalar (corresponds to the bias) &quot;&quot;&quot; w = rand(Float64, (dim, 1)) b = 0 @assert(size(w) == (dim, 1)) @assert(isa(b, Float64) || isa(b, Int64)) return w, b end . initialize (generic function with 1 method) . Notation . According to our notation, X is of shape (num_features, num_examples), in our case that is (4, 343). So, we need to reshape our X. | m is the number of tranining examples. | Similarly, y is a row vector or as Julia likes to call it Array{Float64, 2} of shape (1, num_examples). | . X = reshape(X, (size(X, 2), size(X, 1))) size(X) . (4, 343) . m = size(X, 2) . 343 . y = reshape(y, (1, size(y, 1))) . 1√ó343 Array{Float64,2}: 0.0 1.0 1.0 0.0 0.0 1.0 0.0 0.0 ‚Ä¶ 0.0 1.0 0.0 1.0 1.0 0.0 1.0 . cost = Array{Float64, 2}(undef, 343, 1); . Forward and Backward propagation . propagate function is the function at the heart of the algorithm. This does the forward prop -&gt; calculate cost -&gt; back-prop. . Forward Propagation: . You get X | You compute $A = sigma(w^T X + b) = (a^{(1)}, a^{(2)}, ..., a^{(m-1)}, a^{(m)})$ | You calculate the cost function: $J = - frac{1}{m} sum_{i=1}^{m}y^{(i)} log(a^{(i)})+(1-y^{(i)}) log(1-a^{(i)})$ | . Here are the two formulas you will be using: . $$ frac{ partial J}{ partial w} = frac{1}{m}X(A-Y)^T tag{7}$$ $$ frac{ partial J}{ partial b} = frac{1}{m} sum_{i=1}^m (a^{(i)}-y^{(i)}) tag{8}$$ . function propagate(w, b, X, Y) &quot;&quot;&quot; Implement the cost function and its gradient for the propagation explained above Arguments: w -- weights, a numpy array of size (num_px * num_px * 3, 1) b -- bias, a scalar X -- data of size (num_px * num_px * 3, number of examples) Y -- true &quot;label&quot; vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples) Return: cost -- negative log-likelihood cost for logistic regression dw -- gradient of the loss with respect to w, thus same shape as w db -- gradient of the loss with respect to b, thus same shape as b Tips: - Write your code step by step for the propagation &quot;&quot;&quot; # Forward prop Z = w&#39;X .+ b A = œÉ.(Z) @assert(size(A) == size(y)) # Compute cost ùí• = -1 * sum(y .* log.(A) .+ (1 .- y) .* log.(1 .- A)) ùí• /= m # Back-prop dz = A - Y @assert(size(dz) == size(A) &amp;&amp; size(dz) == size(Y)) dw = (1/m) * X * dz&#39; db = (1/m) * sum(dz) ùí•, dw, db end . propagate (generic function with 1 method) . w, b = initialize(size(X, 1)) . ([0.2004128179916913; 0.7269986373814084; 0.7116663451002054; 0.6867182335637176], 0) . Model . Combine all functions to train the model Learning rate: $ alpha = 0.09$, iterations(epochs): 150 . Here is something I love about Julia. It&#39;s that you can directly use symbols as variables üòç. Doesn&#39;t it look awesome? . #collapse-hide Œ± = 0.009 cost = Array{Float64, 2}(undef, 150, 1) for i=1:150 ùí•, dw, db = propagate(w, b, X, y) cost[i] = ùí• global w, b w -= Œ± * dw; b -= Œ± * db; println(&quot;cost after iteration $i: $ùí•&quot;) end . . cost after iteration 1: 2.3740023073890866 cost after iteration 2: 2.3462116695406623 cost after iteration 3: 2.318509301558028 cost after iteration 4: 2.2908975633354554 cost after iteration 5: 2.2633788921961275 cost after iteration 6: 2.235955804801441 cost after iteration 7: 2.2086308999424062 cost after iteration 8: 2.1814068611217134 cost after iteration 9: 2.154286458914731 cost after iteration 10: 2.127272553920777 cost after iteration 11: 2.1003680993236715 cost after iteration 12: 2.0735761437103326 cost after iteration 13: 2.0468998340349147 cost after iteration 14: 2.02034241843537 cost after iteration 15: 1.9939072492856498 cost after iteration 16: 1.9675977862410752 cost after iteration 17: 1.9414175993047091 cost after iteration 18: 1.9153703719657098 cost after iteration 19: 1.889459904350958 cost after iteration 20: 1.8636901163469042 cost after iteration 21: 1.8380650507400902 cost after iteration 22: 1.8125888762724895 cost after iteration 23: 1.7872658906347527 cost after iteration 24: 1.7621005233341813 cost after iteration 25: 1.7370973384132224 cost after iteration 26: 1.712261036954804 cost after iteration 27: 1.687596459348754 cost after iteration 28: 1.6631085872491747 cost after iteration 29: 1.6388025451803738 cost after iteration 30: 1.6146836017227515 cost after iteration 31: 1.590757170225785 cost after iteration 32: 1.567028808982005 cost after iteration 33: 1.5435042207992091 cost after iteration 34: 1.5201892519078846 cost after iteration 35: 1.4970898901396426 cost after iteration 36: 1.4742122623148985 cost after iteration 37: 1.4515626307794567 cost after iteration 38: 1.4291473890317374 cost after iteration 39: 1.4069730563873706 cost after iteration 40: 1.3850462716301533 cost after iteration 41: 1.3633737856049706 cost after iteration 42: 1.3419624527121798 cost after iteration 43: 1.320819221270383 cost after iteration 44: 1.299951122719665 cost after iteration 45: 1.279365259645181 cost after iteration 46: 1.2590687926069641 cost after iteration 47: 1.2390689257689431 cost after iteration 48: 1.219372891326446 cost after iteration 49: 1.1999879327368639 cost after iteration 50: 1.180921286762925 cost after iteration 51: 1.1621801643408496 cost after iteration 52: 1.1437717302868156 cost after iteration 53: 1.1257030818538676 cost after iteration 54: 1.1079812261472688 cost after iteration 55: 1.090613056399182 cost after iteration 56: 1.073605327093288 cost after iteration 57: 1.0569646279166243 cost after iteration 58: 1.040697356500272 cost after iteration 59: 1.024809689893287 cost after iteration 60: 1.00930755469731 cost after iteration 61: 0.9941965957744495 cost after iteration 62: 0.9794821434313243 cost after iteration 63: 0.9651691789807894 cost after iteration 64: 0.9512622985935968 cost after iteration 65: 0.937765675379124 cost after iteration 66: 0.9246830196811211 cost after iteration 67: 0.9120175376445453 cost after iteration 68: 0.8997718882052996 cost after iteration 69: 0.8879481387767842 cost after iteration 70: 0.8765477200543355 cost after iteration 71: 0.8655713805269151 cost after iteration 72: 0.8550191414681693 cost after iteration 73: 0.8448902533664484 cost after iteration 74: 0.8351831549332133 cost after iteration 75: 0.8258954359868429 cost after iteration 76: 0.8170238056283772 cost after iteration 77: 0.8085640671913646 cost after iteration 78: 0.8005111014454732 cost after iteration 79: 0.7928588594516951 cost after iteration 80: 0.7856003662995654 cost after iteration 81: 0.7787277367034907 cost after iteration 82: 0.7722322031028225 cost after iteration 83: 0.766104156512534 cost after iteration 84: 0.7603331999287829 cost after iteration 85: 0.7549082136319242 cost after iteration 86: 0.7498174312775551 cost after iteration 87: 0.7450485252532479 cost after iteration 88: 0.7405886994319086 cost after iteration 89: 0.7364247871943844 cost after iteration 90: 0.7325433524392283 cost after iteration 91: 0.7289307912533028 cost after iteration 92: 0.7255734319813323 cost after iteration 93: 0.722457631595763 cost after iteration 94: 0.7195698665139034 cost after iteration 95: 0.716896816316328 cost after iteration 96: 0.7144254391657412 cost after iteration 97: 0.7121430380857576 cost after iteration 98: 0.7100373176131921 cost after iteration 99: 0.7080964306677431 cost after iteration 100: 0.7063090157759134 cost after iteration 101: 0.7046642250328173 cost after iteration 102: 0.7031517433816896 cost after iteration 103: 0.701761799935867 cost after iteration 104: 0.700485172164231 cost after iteration 105: 0.6993131838133823 cost after iteration 106: 0.6982376974542951 cost after iteration 107: 0.6972511025247742 cost after iteration 108: 0.6963462996986156 cost after iteration 109: 0.6955166823545526 cost after iteration 110: 0.6947561158487165 cost after iteration 111: 0.6940589152185364 cost after iteration 112: 0.6934198218678441 cost after iteration 113: 0.6928339797057265 cost after iteration 114: 0.6922969111377242 cost after iteration 115: 0.6918044932390484 cost after iteration 116: 0.6913529343765882 cost after iteration 117: 0.690938751490183 cost after iteration 118: 0.6905587481941259 cost after iteration 119: 0.6902099938169665 cost after iteration 120: 0.6898898034611582 cost after iteration 121: 0.6895957191333673 cost after iteration 122: 0.6893254919709155 cost after iteration 123: 0.6890770655691838 cost after iteration 124: 0.6888485603983463 cost after iteration 125: 0.6886382592849551 cost after iteration 126: 0.6884445939241216 cost after iteration 127: 0.688266132380874 cost after iteration 128: 0.6881015675342764 cost after iteration 129: 0.6879497064146782 cost after iteration 130: 0.6878094603827019 cost after iteration 131: 0.6876798360979788 cost after iteration 132: 0.6875599272259485 cost after iteration 133: 0.6874489068320602 cost after iteration 134: 0.6873460204142585 cost after iteration 135: 0.6872505795265694 cost after iteration 136: 0.6871619559488045 cost after iteration 137: 0.6870795763597692 cost after iteration 138: 0.6870029174738363 cost after iteration 139: 0.6869315016032344 cost after iteration 140: 0.6868648926109056 cost after iteration 141: 0.6868026922212128 cost after iteration 142: 0.6867445366581466 cost after iteration 143: 0.6866900935829536 cost after iteration 144: 0.6866390593052681 cost after iteration 145: 0.6865911562438682 cost after iteration 146: 0.6865461306151157 cost after iteration 147: 0.6865037503289286 cost after iteration 148: 0.6864638030738268 cost after iteration 149: 0.6864260945741462 cost after iteration 150: 0.6863904470039756 . Plotting . For plotting we use the Plots package. . x = 1:150; y = cost; gr() # backend plot(x, y, title = &quot;Cost v/s iteration&quot;, label=&quot;negative log-likelihood&quot;, lw=3) xlabel!(&quot;iteration&quot;) ylabel!(&quot;cost&quot;) . Github repo for notebook: Julia_ML . As always, thank you for reading üòäüòÉ! .",
            "url": "abhishekswain.me/machine%20learning/maths/2020/07/28/Logistic_regression.html",
            "relUrl": "/machine%20learning/maths/2020/07/28/Logistic_regression.html",
            "date": " ‚Ä¢ Jul 28, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Statistics and Linear Algebra for ML/DL",
            "content": "Hey there! üëãüòÉ . Today, I am gonna talk about something I have been meaning to do for the longest amout of time. I am now many months into my ML journey, so I have now quite a bit of experience. So, I have a basic understanding of the ML/DL algos but it always feels like I lack in explaining why something works over the others. This is where the mathematical part comes in. Having a sound understanding of the statistical methods is necessary when the explainability comes in. Inorder to do that I have decided to start a Probability &amp; Statistics and Linear Algebra series. This way I would not only be able to learn both but also share with the community which gave me so much. Also, it‚Äôs a part of the ‚ÄúFeynmann technique‚Äù which I am so fond of. . Probability &amp; Statistics . Books: . Business statistics for contemporary decision making | Mathematical Statistics with applications | All of statistics | . Video lectures: . Harvard Stats 110 | Professor Leonard‚Äôs lectures | Larry Wasserman‚Äôs lectures | . Linear Algebra . Books: . Introduction to Linear Algebra (Gilbert Strang) | . Video lectures: . Gilbert Strang lectures | . Applied resources . My favorite: An Introduction to Statistical Learning: With Applications in R | Practical Statistics for Data Scientists: 50+ Essential Concepts Using R and Python | Think Stats(Learn stats using Python) | Introduction to Linear Algebra for Applied Machine Learning with Python | Peter Norvig‚Äôs: A Concrete Introduction to Probability (using Python) | . As I read the material and understand a concept I will write an article about it. It will be two parellel series one for each. If you want to follow it then all of it will be posted on my Medium. I will see you there! . As always, thank you for reading üòäüòÉ! . Update(21/07/2020) . I need to strike a balance between theory and application. I actually read few chapters of the book but found that I am more into the code-first approach to learning. So, there‚Äôs a slight modification to my plan. For linear algebra, I am using Computational linear algebra by Rachel Thomas. I have just begun and I already like it. I will use it parellely with Glibert Strang‚Äôs linear algebra course. . For proabability &amp; stats: Think Stats + any theory course mentioned. .",
            "url": "abhishekswain.me/machine%20learning/maths/books/2020/07/19/statsandlinalg.html",
            "relUrl": "/machine%20learning/maths/books/2020/07/19/statsandlinalg.html",
            "date": " ‚Ä¢ Jul 19, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Introduction to Machine Learning course (Refreshing the maths)",
            "content": "Hey there! üëãüòÉ . Today I want to talk about revisiting Machine learning especailly with a focus on underlying mathematics. Well let me tell you, I have just found the perfect course for it. It‚Äôs the Introduction to Machine Learning course. It‚Äôs taught by Prof Balaraman Ravindran. He is someone from whom I have really learnt a lot. And infact the course is by IIT Madras, it‚Äôs pretty close to my heart because my introduction to machine learning class almost a year ago was by IIT Madras, not this one but this one ü§ó. I always recommend it to people. Something I‚Äôm really in love with these days is fastai ü§óü§ó. I am really in love with Jeremy‚Äôs top-down method of teaching. The best thing abouut fastai is you build the whole of PyTorch from ground up. It just doesn‚Äôt teach you Deep Learning, but also makes you a better Python programmer. I recommend it to everyone. Fastai also has ML and Computational linear algebra courses. I haven‚Äôt gone through them but I bet they are as awesome as the Deep learning one. . Anyways, I have enrolled myself into the NPTEL ML course, so that it can be a refresher. I have my semesters coming up and it starts from the day my sems begin that is 20 &amp; also the whole TCS onboarding to follow but, come on when did we go by the convention? Ofcourse amid all the chaos I am gonna try, that‚Äôs how we roll. Let‚Äôs see how it turns out! As always anything and evrything I do I will update it here. Also, I was really confused about what to put on Medium and what here?. Needed to draw a line. I have decided that all tutorials, build &amp; learning journeys go there. Everything except that goes here! Let the sems just get over &amp; the fun will begin. . I want to explore so many things and learn so much. Nothing makes me happier than sharing it with the community that I learnt from. ü§ó And teaching is the best way I have found to effectively learn things. . As always, thank you for reading üòäüòÉ! . (UPDATE 19/07/2020) . . The course was supposed to start from tomorrow but it has got rescheduled to semptember 14. Damn! that is 2 months away. So, need a change of plans. Actually, I had another plan in my mind for the whole maths refresher and getting to know the nitty gritty. The next post will be all about it. . As always, thank you for reading üòäüòÉ! .",
            "url": "abhishekswain.me/machine%20learning/2020/07/14/nptelML.html",
            "relUrl": "/machine%20learning/2020/07/14/nptelML.html",
            "date": " ‚Ä¢ Jul 14, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Rust v/s C++ for Deep learning",
            "content": "Hey there! üëã Today, let‚Äôs discuss about Rust v/s C++. Actually, it‚Äôs not ‚ÄúRust v/s C++‚Äù as in comparison of the languages, but I just want a language for understanding the underpinnings of deep learning algorithms. I want to build them from scratch or atleast try, so that I have a better understanding. One of my favorite physicists, Richard Feynman said something I resonate with a lot: . ‚ÄúWhat I cannot create, I do not understand.‚Äù . So, my search for a language begun. Let me tell you this, I really love Python. It‚Äôs one language I really am very comfortable with and Python was what I used to get into data science (not that there were many others). Python was simple and easy. However, I wanted to do some low level programming. Enter C++. C was my first language way back in school, then C++ &amp; Java. However, it has been years since I actually did something worthwhile with them. I did use Java in my undergrad but it wasn‚Äôt as much. Then came Python which I learned as I wanted to get into DS, ML. And I fell in love!. After working with python for many months, one day I discovered that Python is not what is used in deployment! I was like what? really? So, then what? . I found C++ was what most production systems used. In my CS undergrad we had a subject called Systems programming in which we wrote assembly code on IBM 360. I really liked it, it was a pain in the ass to do things as simple as addition, substraction but it really made me understand how the code gets compiled and the internals of it. This started a love with low level programming, I always wanted to do build things from scratch but never got around to do it. It is something I wish I had done! I guess I never thought I could actually do it. But, life is too short for it. Now is the time I finally think of doing it. . Okay, all that aside, I now have narrowed down my choices to Rust &amp; C++. There are other like Swift and Julia. Maybe Julia is more suited to this. But as I said I want to do it with a low level programming language. It would be shit-hard, but I want to challenge myself and if I can do atleast half of it, this would increase my confidence ten-fold. Sundar Pichai in one of his interview said, . ‚ÄúTry solving the hardest problem first, if you can then the easier ones will follow‚Äù. . I did extensive research and found: If I use C++, there‚Äôs a job market out there where I can transfer my C++ skills. On the other hand Rust is a pretty young language. But the thing is I gravitate more to Rust &amp; the heart wants what it wants. Naturally, I went through all the blogs and reddit. There were two groups of people. The first kind termed rust as a revolution, as something they called a memory safe language and gave C++ a lot of flak. The second ones said it‚Äôs never gonna replace C &amp; C++ anytime soon, pretty correct actually since a lot of code bases is all C &amp; C++. But, code safety with what I was able to understand is basically just bad code. Rust provides a stricter compliler check like you can‚Äôt simple pass values around. So what do I do? . I will take my chance &amp; go with Rust ‚ù§ . As always, thank you for reading üòäüòÉ! . (UPDATE) . Wasn‚Äôt as simple as I thought üòÖ . This is an update on the Rust v/s C++ post. It‚Äôs been hours I have been hacking through, reading everything I can find about Rust in ML/DL. It won‚Äôt be hard to say that most of them are abandoned. Some are still on but it seems like, there‚Äôs no way they are comaprable to C++ alternatives. Without much of a community around, it‚Äôs been pretty hard to get through anything or maybe I haven‚Äôt searched enough. I also had a discussion with my friends who are into ML &amp; they suggested me to go for C++. Let‚Äôs come to what are the current examples in Rust. The best one I found was huggingface tokenizers. There were also crates like tch-rs which provide Rust bindings for C++ torch api. . So, what is the state of rust in ML/DL? It‚Äôs progressing slowly and steadily, but way further from maturity. It‚Äôs very young and I guess someday we will be able to use Rust for ML/DL, but as of now I think am going to give it a pass. I know it feels like giving up, trust me even I felt so, but the fact is I will be working a full time job as a SDE at TCS soon. With a full time job, I need to be more cautious with how I use my time or spend my energies. While it‚Äôs a lovely idea to explore my fantasies but I guess I should be productive at the same time. Also, I will be working with java alot at TCS so that would be like one more langugae added to the mix. In the end, there‚Äôs no effective learning. Lately, by thinking through things I have learnt that I tend to go for new things and fancy stuff. But actually that‚Äôs not what‚Äôs important. Whatever you do, you need rock solid fundamentals. Tooling is trivial as long as you really know what you‚Äôre doing. So, I will be giving Rust a pass for now. But definitely one day I will come back to visit rust once again. Till then rather than learning a new language, I will stick what I have atleast some experience with, .i.e. Python, C++ or Java . As always, thank you for reading üòäüòÉ! .",
            "url": "abhishekswain.me/deep%20learning/machine%20learning/programming%20language/2020/07/12/rust-vs-cpp.html",
            "relUrl": "/deep%20learning/machine%20learning/programming%20language/2020/07/12/rust-vs-cpp.html",
            "date": " ‚Ä¢ Jul 12, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Importance of a proper posture",
            "content": "Today I want to talk about something I had a really bad time with. You must have already guessed from the title, I am going to talk to you about why it‚Äôs important to maintain a good posture while doing whatever is that you do, but I will talk to you mostly from programmer point of view. . If you are like me who loves to learn stuff on your own all the time, especailly things like coding, &amp; mostly CS related stuff you must be spending a lot of time infront of you laptop or PC. Now and then I have seen I struggle a lot with a lot of neck pain. So, finally I decided to do a little bit of research. I found out that if my laptop was on level with my eyes or a little above, then I could easily sit for a long time. On the other hand if it was below my eye level and I tried to hunch over and do stuff 7 out of 10 times I would have a stiff neck. And as I am writing this, I do have stiffness in my neck. It‚Äôs very painful. And I don‚Äôt want anyone else to suffer from this. So, let me tell you what I have been doing, it‚Äôs not much but I hope it will be of some use: . Watch this amazing video by a physiotherapist: | It did me a world of good especially the last exercise. . Apply pain relieving spray or apply balm, I applied omnigel | There are also really good articles explaining why it happens and how you can remedy it. Like this one: Neck Pain: Possible Causes and How to Treat It &amp; The Importance of Posture. . I would also like to tell you what one of my favorite school principals sister Innocentia told me, ‚ÄúNever take a chance with your health, if you‚Äôre healthy then things can be done at a later time. But if you cripple yourself today maybe you won‚Äôt be able to do it ever again‚Äù. . Thank you for reading ! .",
            "url": "abhishekswain.me/health/lifestyle/2020/07/10/importance-of-posture.html",
            "relUrl": "/health/lifestyle/2020/07/10/importance-of-posture.html",
            "date": " ‚Ä¢ Jul 10, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "questus ordo in rebus",
            "content": "This is my first post here and I wanted it to be something I have wanted to do for the longest time. questus ordo in rebus is latin for getting things in order. I was very naive to think that it was pretty simple to do and maybe it is for many people but guess I belong to the latter category where you gotta struggle to do it. Well the bottom line is, I want to get things in order. It‚Äôs not like I have not tried to do it before but there was just so much happening around me, I mean from classes, seminar, minor project to placements, major presentation etc that I couldn‚Äôt pause to think about where I am in life and what I want to do moving forward. So, this lockdown period for me was like brakes applied to a car moving at a great speed, it‚Äôs kind of pretty sudden and shakes you up but if you think about it maybe it was required?. I think that was the worst analogy I could come up with, but I really am not very good with words. Basically, this lockdown got me pausing and thinking. I wont lie, it was rather frustrating in the beginning because I was used to go walking, running &amp; stuff I did because I wanted to be a ‚Äúfit üí™ guy‚Äù but now that it has been quite sometime I feel it has done me a lot of good. Like I said maybe the brakes were required?. So, let‚Äôs begin ! . I feel like things I have learnt I know them pretty well but they seem to be in scatters. Well, not anymore, that ends today. Something which I love to do is read &amp; read a lot. My school notebooks had this thing writetn on the back cover: . ‚ÄúReading maketh a full man.‚Äù . I used to think it was someone in my school who wrote it, but it actually is a quote by Francis Bacon. It resonates with me alot. Reading actually fills your mind with a variety, so the next time you think about anything you look at it from a lot perspectives. And that is where we are going to start today. I have a lot of machine learning, deep learning, statistics, data science books, now this is very cool actually but it also is very overwhelming. Like I get really confused as to where to start. I want to read everything and I try to do it all at once and I end up not reading anything. After screwing up numerous times I have a plan of what I want to do. I have decided I will read one applied book along with a more theoretical book. . The options: . Applied machine learning &amp; deep learning: Approaching almost any machine learning problem | Hands-on machine learning with scikit-learn &amp; tensorflow | Python machine learning by sebestian raschka | Programming collective intelligence | . | Theoretical books: PRML | BRML | ESLR | ISLR(has code examples in R) | Professor Michael I. Jordan‚Äôs reading list | &amp; list continues‚Ä¶‚Ä¶‚Ä¶‚Ä¶.. | . | . So, what are my choices? . After a lot of research I have decided to go for Hands-on ML &amp; for theory I chose ESLR. I will give my best to it and try and keep posting here about my progress. . Wait a minute! Aren‚Äôt we missing something? The younger me would have stopped right here but, I now am a little sensible and I realise no amount of book reading will do me any good if I don‚Äôt code and build things up. Enter Kaggle. Besides reading stuff I will try and spend my time implementing things in kaggle! Let‚Äôs see how it goes! See ya. . PS: I have another dream of building a deep learning library using rust, I really wanna learn about it. That‚Äôs for another post ! . Thank you for reading ! .",
            "url": "abhishekswain.me/roadmap/books/2020/07/09/roadmap.html",
            "relUrl": "/roadmap/books/2020/07/09/roadmap.html",
            "date": " ‚Ä¢ Jul 9, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": ". Hello! I am Abhishek, and this is my blog. Everything I have ever known about the world around me is by listening to life with all my attention. This blog is like an extension of me. I welcome you all to ‚ÄúListening to Life‚Äù . I also post on medium . You can reach me on: linkedin . If you want to have a look at my CV: My CV .",
          "url": "abhishekswain.me/aboutme/",
          "relUrl": "/aboutme/",
          "date": ""
      }
      
  

  

  
      ,"page3": {
          "title": "Projects",
          "content": "MyVision: Customizability meets abstraction . This is my Computer Vision toolkit. It‚Äôs a wrapper over PyTorch combining things I like from different libraries like fastai, keras etc. . Leaf disease detection using image segmentation . This is my final year project about detecting plant leaf disease and segmenting the diseased part. However, there‚Äôs a catch in my project, I have done it without the use of any cnn for segmentation. Instead, it uses a cascade of classifiers to do the detection and segmentation. . CheXNet: Radiologist-Level Pneumonia Detection using deep learning . This project is an implementation of the paper I worked on. CheXNet detects 14 different pathologies from chest X-rays. It uses a Densenet-121 for detection. .",
          "url": "abhishekswain.me/projects/",
          "relUrl": "/projects/",
          "date": ""
      }
      
  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ ‚Äúsitemap.xml‚Äù | absolute_url }} | .",
          "url": "abhishekswain.me/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}