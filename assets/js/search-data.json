{
  
    
        "post0": {
            "title": "Hello, Julia!",
            "content": "Hey! This post is about my introduction to the world of Julia. I took this challenge of learning Julia and making something in it. Since Julia is pretty similar to Python, I made a hypothesis. That is can I learn julia and be up and running with something in two days? What I realised is, if you&#39;re from a python background and have some expereince in it, then learning Julia is going to be fun and breezy for you. So, here I am after my two day rendezvous with Julia. . So, what I used to learn Julia? I used resources from julia academy . What did I implement? I decided to go for one the resources I learnt deep learning from: Neural Networks and Deep Learning . Impemented the Julia version of Week 2 assignment of Neural Networks and Deep Learning course. . | It&#39;s not exactly that actually, I have applied the concepts to banknote authentication. . | Data is from UCI machine learning repository | Training is done, inference is on the way! | . I hope it&#39;s useful to you. It was a lot of fun and I am in love with Julia ‚ù§ . Let&#39;s begin! . using DelimitedFiles using Random using Plots . file = DelimitedFiles.readdlm(&quot;C: Users Abhishek Swain Desktop Julia_ML data_banknote_authentication.txt&quot;, &#39;,&#39;); . See the first 5 rows of the given array . function head(file) file[1:5, 1:5] end . head (generic function with 1 method) . head(file) . 5√ó5 Array{Float64,2}: 3.6216 8.6661 -2.8073 -0.44699 0.0 4.5459 8.1674 -2.4586 -1.4621 0.0 3.866 -2.6383 1.9242 0.10645 0.0 3.4566 9.5228 -4.0112 -3.5944 0.0 0.32924 -4.4552 4.5718 -0.9888 0.0 . Normalization . function normalize(file, num_cols) for i=1:num_cols file[:, i] /= sum(file[:, i]) end end . normalize (generic function with 1 method) . normalize(file, 4) . head(file) . 5√ó5 Array{Float64,2}: 0.00608586 0.00328576 -0.00146401 0.000273396 0.0 0.00763908 0.00309668 -0.00128216 0.000894277 0.0 0.00649655 -0.00100032 0.00100347 -6.51089e-5 0.0 0.00580858 0.00361058 -0.00209184 0.00219847 0.0 0.000553266 -0.0016892 0.0023842 0.000604788 0.0 . Splitting into train &amp; test set . splits the data into train and test set . function train_test_split(file, at=0.7) n = size(file, 1) idx = shuffle(1:n) train_idx = view(idx, 1:floor(Int, at*n)) test_idx = view(idx, (floor(Int, at*n)+1):n) file = convert(Matrix, file) return file[train_idx,:], file[test_idx,:] end . train_test_split (generic function with 2 methods) . data_train, data_test = train_test_split(file); X_train, y_train = data_train[:, 1:4], data_train[:, 5]; . Mathematical expression of the algorithm: . For one example $x^{(i)}$: $$z^{(i)} = w^T x^{(i)} + b tag{1}$$ $$ hat{y}^{(i)} = a^{(i)} = sigmoid(z^{(i)}) tag{2}$$ $$ mathcal{L}(a^{(i)}, y^{(i)}) = - y^{(i)} log(a^{(i)}) - (1-y^{(i)} ) log(1-a^{(i)}) tag{3}$$ . The cost is then computed by summing over all training examples: $$ J = frac{1}{m} sum_{i=1}^m mathcal{L}(a^{(i)}, y^{(i)}) tag{6}$$ . Sigmoid . Applies sigmoid to the vector . function œÉ(z) &quot;&quot;&quot; Compute the sigmoid of z &quot;&quot;&quot; return one(z) / (one(z) + exp(-z)) end . œÉ (generic function with 1 method) . Random initialization . Initialize w &amp; b with with random values between (0, 1) . function initialize(dim) &quot;&quot;&quot; This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0. Argument: dim -- size of the w vector we want (or number of parameters in this case) Returns: w -- initialized vector of shape (dim, 1) b -- initialized scalar (corresponds to the bias) &quot;&quot;&quot; w = zeros(dim, 1) b = 2 @assert(size(w) == (dim, 1)) @assert(isa(b, Float64) || isa(b, Int64)) return w, b end . initialize (generic function with 1 method) . Notation . According to our notation, X is of shape (num_features, num_examples), in our case that is (4, 343). So, we need to reshape our X. | m is the number of tranining examples. | Similarly, y is a row vector or as Julia likes to call it Array{Float64, 2} of shape (1, num_examples). | . X_train = reshape(X_train, (size(X_train, 2), size(X_train, 1))) size(X_train) . (4, 960) . m = size(X_train, 2) . 960 . y_train = reshape(y_train, (1, size(y_train, 1))) . 1√ó960 Array{Float64,2}: 0.0 1.0 1.0 1.0 0.0 1.0 1.0 0.0 ‚Ä¶ 1.0 0.0 0.0 1.0 0.0 1.0 1.0 . cost = Array{Float64, 2}(undef, size(X_train, 2), 1); . Forward and Backward propagation . propagate function is the function at the heart of the algorithm. This does the forward prop -&gt; calculate cost -&gt; back-prop. . Forward Propagation: . You get X | You compute $A = sigma(w^T X + b) = (a^{(1)}, a^{(2)}, ..., a^{(m-1)}, a^{(m)})$ | You calculate the cost function: $J = - frac{1}{m} sum_{i=1}^{m}y^{(i)} log(a^{(i)})+(1-y^{(i)}) log(1-a^{(i)})$ | . Here are the two formulas you will be using: . $$ frac{ partial J}{ partial w} = frac{1}{m}X(A-Y)^T tag{7}$$ $$ frac{ partial J}{ partial b} = frac{1}{m} sum_{i=1}^m (a^{(i)}-y^{(i)}) tag{8}$$ . function propagate(w, b, X, Y) &quot;&quot;&quot; Implement the cost function and its gradient for the propagation explained above Arguments: w -- weights, a numpy array of size (num_px * num_px * 3, 1) b -- bias, a scalar X -- data of size (num_px * num_px * 3, number of examples) Y -- true &quot;label&quot; vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples) Return: cost -- negative log-likelihood cost for logistic regression dw -- gradient of the loss with respect to w, thus same shape as w db -- gradient of the loss with respect to b, thus same shape as b Tips: - Write your code step by step for the propagation &quot;&quot;&quot; # Forward prop Z = w&#39;X .+ b A = œÉ.(Z) @assert(size(A) == size(y_train)) # Compute cost ùí• = -1 * sum(y_train .* log.(A) .+ (1 .- y_train) .* log.(1 .- A)) ùí• /= m # Back-prop dz = A - Y @assert(size(dz) == size(A) &amp;&amp; size(dz) == size(Y)) dw = (1/m) * X * dz&#39; db = (1/m) * sum(dz) ùí•, dw, db end . propagate (generic function with 1 method) . w, b = initialize(size(X_train, 1)) . ([0.0; 0.0; 0.0; 0.0], 2) . Model . Combine all functions to train the model Learning rate: $ alpha = 0.09$, iterations(epochs): 150 . Here is something I love about Julia. It&#39;s that you can directly use symbols as variables üòç. Doesn&#39;t it look awesome? . Œ± = 0.01 cost = Array{Float64, 2}(undef, 1500, 1) for i=1:1500 ùí•, dw, db = propagate(w, b, X_train, y_train) cost[i] = ùí• global w, b w -= Œ± * dw; b -= Œ± * db; println(&quot;cost after iteration $i: $ùí•&quot;) end . Plotting . For plotting we use the Plots package. . x = 1:1500; y = cost; gr() # backend plot(x, y, title = &quot;Cost v/s iteration(Training)&quot;, label=&quot;negative log-likelihood&quot;, lw=3) xlabel!(&quot;iteration&quot;) ylabel!(&quot;cost&quot;) . function predict(w, b, X) &quot;&quot;&quot; Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b) Arguments: w -- weights, a numpy array of size (num_px * num_px * 3, 1) b -- bias, a scalar X -- data of size (num_px * num_px * 3, number of examples) Returns: Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X &quot;&quot;&quot; m = size(X, 1) Y_pred = zeros(1, m) X = reshape(X, (size(X, 2), size(X, 1))) A = œÉ.(w&#39;X .+ b) for i=1:size(A, 2) if(A[i] &gt; 0.5) Y_pred[i] = 1 else Y_pred[i] = 0 end end @assert(size(Y_pred) == (1, m)) Y_pred end . predict (generic function with 1 method) . X_test, y_test = data_test[:, 1:4], data_test[:, 5]; . preds = predict(w, b, X_test) . 1√ó412 Array{Float64,2}: 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ‚Ä¶ 0.0 0.0 0.0 0.0 0.0 0.0 0.0 . y_test = reshape(y_test, (1, 412)) . 1√ó412 Array{Float64,2}: 0.0 1.0 0.0 0.0 1.0 0.0 1.0 1.0 ‚Ä¶ 0.0 0.0 1.0 1.0 1.0 0.0 0.0 . cnt = 0 for i=1:size(y_test, 2) if(y_test[i] == preds[i]) cnt += 1 end end . acc = (cnt/size(y_test, 2)) * 100; . println(&quot;Accuracy: $acc&quot;) . Accuracy: 55.582524271844655 . Epilouge . You can see accuracy is really just 56% which is what to say ü§£ü§£, but then I have not done any preprocessing except normalization &amp; then there are also things we can do to optimize, like handling class-imbalance etc. . . The goal was to implement Logistic regression with back-prop from scratch which I did. Hope you will take something from it. . Github repo for notebook: Julia_ML . As always, thank you for reading üòäüòÉ! .",
            "url": "abhishekswain.me/machine%20learning/maths/2020/07/28/Logistic_regression.html",
            "relUrl": "/machine%20learning/maths/2020/07/28/Logistic_regression.html",
            "date": " ‚Ä¢ Jul 28, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Statistics and Linear Algebra for ML/DL",
            "content": "Hey there! üëãüòÉ . Today, I am gonna talk about something I have been meaning to do for the longest amout of time. I am now many months into my ML journey, so I have now quite a bit of experience. So, I have a basic understanding of the ML/DL algos but it always feels like I lack in explaining why something works over the others. This is where the mathematical part comes in. Having a sound understanding of the statistical methods is necessary when the explainability comes in. Inorder to do that I have decided to start a Probability &amp; Statistics and Linear Algebra series. This way I would not only be able to learn both but also share with the community which gave me so much. Also, it‚Äôs a part of the ‚ÄúFeynmann technique‚Äù which I am so fond of. . Probability &amp; Statistics . Books: . Business statistics for contemporary decision making | Mathematical Statistics with applications | All of statistics | . Video lectures: . Harvard Stats 110 | Professor Leonard‚Äôs lectures | Larry Wasserman‚Äôs lectures | . Linear Algebra . Books: . Introduction to Linear Algebra (Gilbert Strang) | . Video lectures: . Gilbert Strang lectures | . Applied resources . My favorite: An Introduction to Statistical Learning: With Applications in R | Practical Statistics for Data Scientists: 50+ Essential Concepts Using R and Python | Think Stats(Learn stats using Python) | Introduction to Linear Algebra for Applied Machine Learning with Python | Peter Norvig‚Äôs: A Concrete Introduction to Probability (using Python) | . As I read the material and understand a concept I will write an article about it. It will be two parellel series one for each. If you want to follow it then all of it will be posted on my Medium. I will see you there! . As always, thank you for reading üòäüòÉ! . Update(21/07/2020) . I need to strike a balance between theory and application. I actually read few chapters of the book but found that I am more into the code-first approach to learning. So, there‚Äôs a slight modification to my plan. For linear algebra, I am using Computational linear algebra by Rachel Thomas. I have just begun and I already like it. I will use it parellely with Glibert Strang‚Äôs linear algebra course. . For proabability &amp; stats: Think Stats + any theory course mentioned. .",
            "url": "abhishekswain.me/machine%20learning/maths/books/2020/07/19/statsandlinalg.html",
            "relUrl": "/machine%20learning/maths/books/2020/07/19/statsandlinalg.html",
            "date": " ‚Ä¢ Jul 19, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Introduction to Machine Learning course (Refreshing the maths)",
            "content": "Hey there! üëãüòÉ . Today I want to talk about revisiting Machine learning especailly with a focus on underlying mathematics. Well let me tell you, I have just found the perfect course for it. It‚Äôs the Introduction to Machine Learning course. It‚Äôs taught by Prof Balaraman Ravindran. He is someone from whom I have really learnt a lot. And infact the course is by IIT Madras, it‚Äôs pretty close to my heart because my introduction to machine learning class almost a year ago was by IIT Madras, not this one but this one ü§ó. I always recommend it to people. Something I‚Äôm really in love with these days is fastai ü§óü§ó. I am really in love with Jeremy‚Äôs top-down method of teaching. The best thing abouut fastai is you build the whole of PyTorch from ground up. It just doesn‚Äôt teach you Deep Learning, but also makes you a better Python programmer. I recommend it to everyone. Fastai also has ML and Computational linear algebra courses. I haven‚Äôt gone through them but I bet they are as awesome as the Deep learning one. . Anyways, I have enrolled myself into the NPTEL ML course, so that it can be a refresher. I have my semesters coming up and it starts from the day my sems begin that is 20 &amp; also the whole TCS onboarding to follow but, come on when did we go by the convention? Ofcourse amid all the chaos I am gonna try, that‚Äôs how we roll. Let‚Äôs see how it turns out! As always anything and evrything I do I will update it here. Also, I was really confused about what to put on Medium and what here?. Needed to draw a line. I have decided that all tutorials, build &amp; learning journeys go there. Everything except that goes here! Let the sems just get over &amp; the fun will begin. . I want to explore so many things and learn so much. Nothing makes me happier than sharing it with the community that I learnt from. ü§ó And teaching is the best way I have found to effectively learn things. . As always, thank you for reading üòäüòÉ! . (UPDATE 19/07/2020) . . The course was supposed to start from tomorrow but it has got rescheduled to semptember 14. Damn! that is 2 months away. So, need a change of plans. Actually, I had another plan in my mind for the whole maths refresher and getting to know the nitty gritty. The next post will be all about it. . As always, thank you for reading üòäüòÉ! .",
            "url": "abhishekswain.me/machine%20learning/2020/07/14/nptelML.html",
            "relUrl": "/machine%20learning/2020/07/14/nptelML.html",
            "date": " ‚Ä¢ Jul 14, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Rust v/s C++ for Deep learning",
            "content": "Hey there! üëã Today, let‚Äôs discuss about Rust v/s C++. Actually, it‚Äôs not ‚ÄúRust v/s C++‚Äù as in comparison of the languages, but I just want a language for understanding the underpinnings of deep learning algorithms. I want to build them from scratch or atleast try, so that I have a better understanding. One of my favorite physicists, Richard Feynman said something I resonate with a lot: . ‚ÄúWhat I cannot create, I do not understand.‚Äù . So, my search for a language begun. Let me tell you this, I really love Python. It‚Äôs one language I really am very comfortable with and Python was what I used to get into data science (not that there were many others). Python was simple and easy. However, I wanted to do some low level programming. Enter C++. C was my first language way back in school, then C++ &amp; Java. However, it has been years since I actually did something worthwhile with them. I did use Java in my undergrad but it wasn‚Äôt as much. Then came Python which I learned as I wanted to get into DS, ML. And I fell in love!. After working with python for many months, one day I discovered that Python is not what is used in deployment! I was like what? really? So, then what? . I found C++ was what most production systems used. In my CS undergrad we had a subject called Systems programming in which we wrote assembly code on IBM 360. I really liked it, it was a pain in the ass to do things as simple as addition, substraction but it really made me understand how the code gets compiled and the internals of it. This started a love with low level programming, I always wanted to do build things from scratch but never got around to do it. It is something I wish I had done! I guess I never thought I could actually do it. But, life is too short for it. Now is the time I finally think of doing it. . Okay, all that aside, I now have narrowed down my choices to Rust &amp; C++. There are other like Swift and Julia. Maybe Julia is more suited to this. But as I said I want to do it with a low level programming language. It would be shit-hard, but I want to challenge myself and if I can do atleast half of it, this would increase my confidence ten-fold. Sundar Pichai in one of his interview said, . ‚ÄúTry solving the hardest problem first, if you can then the easier ones will follow‚Äù. . I did extensive research and found: If I use C++, there‚Äôs a job market out there where I can transfer my C++ skills. On the other hand Rust is a pretty young language. But the thing is I gravitate more to Rust &amp; the heart wants what it wants. Naturally, I went through all the blogs and reddit. There were two groups of people. The first kind termed rust as a revolution, as something they called a memory safe language and gave C++ a lot of flak. The second ones said it‚Äôs never gonna replace C &amp; C++ anytime soon, pretty correct actually since a lot of code bases is all C &amp; C++. But, code safety with what I was able to understand is basically just bad code. Rust provides a stricter compliler check like you can‚Äôt simple pass values around. So what do I do? . I will take my chance &amp; go with Rust ‚ù§ . As always, thank you for reading üòäüòÉ! . (UPDATE) . Wasn‚Äôt as simple as I thought üòÖ . This is an update on the Rust v/s C++ post. It‚Äôs been hours I have been hacking through, reading everything I can find about Rust in ML/DL. It won‚Äôt be hard to say that most of them are abandoned. Some are still on but it seems like, there‚Äôs no way they are comaprable to C++ alternatives. Without much of a community around, it‚Äôs been pretty hard to get through anything or maybe I haven‚Äôt searched enough. I also had a discussion with my friends who are into ML &amp; they suggested me to go for C++. Let‚Äôs come to what are the current examples in Rust. The best one I found was huggingface tokenizers. There were also crates like tch-rs which provide Rust bindings for C++ torch api. . So, what is the state of rust in ML/DL? It‚Äôs progressing slowly and steadily, but way further from maturity. It‚Äôs very young and I guess someday we will be able to use Rust for ML/DL, but as of now I think am going to give it a pass. I know it feels like giving up, trust me even I felt so, but the fact is I will be working a full time job as a SDE at TCS soon. With a full time job, I need to be more cautious with how I use my time or spend my energies. While it‚Äôs a lovely idea to explore my fantasies but I guess I should be productive at the same time. Also, I will be working with java alot at TCS so that would be like one more langugae added to the mix. In the end, there‚Äôs no effective learning. Lately, by thinking through things I have learnt that I tend to go for new things and fancy stuff. But actually that‚Äôs not what‚Äôs important. Whatever you do, you need rock solid fundamentals. Tooling is trivial as long as you really know what you‚Äôre doing. So, I will be giving Rust a pass for now. But definitely one day I will come back to visit rust once again. Till then rather than learning a new language, I will stick what I have atleast some experience with, .i.e. Python, C++ or Java . As always, thank you for reading üòäüòÉ! .",
            "url": "abhishekswain.me/deep%20learning/machine%20learning/programming%20language/2020/07/12/rust-vs-cpp.html",
            "relUrl": "/deep%20learning/machine%20learning/programming%20language/2020/07/12/rust-vs-cpp.html",
            "date": " ‚Ä¢ Jul 12, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Importance of a proper posture",
            "content": "Today I want to talk about something I had a really bad time with. You must have already guessed from the title, I am going to talk to you about why it‚Äôs important to maintain a good posture while doing whatever is that you do, but I will talk to you mostly from programmer point of view. . If you are like me who loves to learn stuff on your own all the time, especailly things like coding, &amp; mostly CS related stuff you must be spending a lot of time infront of you laptop or PC. Now and then I have seen I struggle a lot with a lot of neck pain. So, finally I decided to do a little bit of research. I found out that if my laptop was on level with my eyes or a little above, then I could easily sit for a long time. On the other hand if it was below my eye level and I tried to hunch over and do stuff 7 out of 10 times I would have a stiff neck. And as I am writing this, I do have stiffness in my neck. It‚Äôs very painful. And I don‚Äôt want anyone else to suffer from this. So, let me tell you what I have been doing, it‚Äôs not much but I hope it will be of some use: . Watch this amazing video by a physiotherapist: | It did me a world of good especially the last exercise. . Apply pain relieving spray or apply balm, I applied omnigel | There are also really good articles explaining why it happens and how you can remedy it. Like this one: Neck Pain: Possible Causes and How to Treat It &amp; The Importance of Posture. . I would also like to tell you what one of my favorite school principals sister Innocentia told me, ‚ÄúNever take a chance with your health, if you‚Äôre healthy then things can be done at a later time. But if you cripple yourself today maybe you won‚Äôt be able to do it ever again‚Äù. . Thank you for reading ! .",
            "url": "abhishekswain.me/health/lifestyle/2020/07/10/importance-of-posture.html",
            "relUrl": "/health/lifestyle/2020/07/10/importance-of-posture.html",
            "date": " ‚Ä¢ Jul 10, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "questus ordo in rebus",
            "content": "This is my first post here and I wanted it to be something I have wanted to do for the longest time. questus ordo in rebus is latin for getting things in order. I was very naive to think that it was pretty simple to do and maybe it is for many people but guess I belong to the latter category where you gotta struggle to do it. Well the bottom line is, I want to get things in order. It‚Äôs not like I have not tried to do it before but there was just so much happening around me, I mean from classes, seminar, minor project to placements, major presentation etc that I couldn‚Äôt pause to think about where I am in life and what I want to do moving forward. So, this lockdown period for me was like brakes applied to a car moving at a great speed, it‚Äôs kind of pretty sudden and shakes you up but if you think about it maybe it was required?. I think that was the worst analogy I could come up with, but I really am not very good with words. Basically, this lockdown got me pausing and thinking. I wont lie, it was rather frustrating in the beginning because I was used to go walking, running &amp; stuff I did because I wanted to be a ‚Äúfit üí™ guy‚Äù but now that it has been quite sometime I feel it has done me a lot of good. Like I said maybe the brakes were required?. So, let‚Äôs begin ! . I feel like things I have learnt I know them pretty well but they seem to be in scatters. Well, not anymore, that ends today. Something which I love to do is read &amp; read a lot. My school notebooks had this thing writetn on the back cover: . ‚ÄúReading maketh a full man.‚Äù . I used to think it was someone in my school who wrote it, but it actually is a quote by Francis Bacon. It resonates with me alot. Reading actually fills your mind with a variety, so the next time you think about anything you look at it from a lot perspectives. And that is where we are going to start today. I have a lot of machine learning, deep learning, statistics, data science books, now this is very cool actually but it also is very overwhelming. Like I get really confused as to where to start. I want to read everything and I try to do it all at once and I end up not reading anything. After screwing up numerous times I have a plan of what I want to do. I have decided I will read one applied book along with a more theoretical book. . The options: . Applied machine learning &amp; deep learning: Approaching almost any machine learning problem | Hands-on machine learning with scikit-learn &amp; tensorflow | Python machine learning by sebestian raschka | Programming collective intelligence | . | Theoretical books: PRML | BRML | ESLR | ISLR(has code examples in R) | Professor Michael I. Jordan‚Äôs reading list | &amp; list continues‚Ä¶‚Ä¶‚Ä¶‚Ä¶.. | . | . So, what are my choices? . After a lot of research I have decided to go for Hands-on ML &amp; for theory I chose ESLR. I will give my best to it and try and keep posting here about my progress. . Wait a minute! Aren‚Äôt we missing something? The younger me would have stopped right here but, I now am a little sensible and I realise no amount of book reading will do me any good if I don‚Äôt code and build things up. Enter Kaggle. Besides reading stuff I will try and spend my time implementing things in kaggle! Let‚Äôs see how it goes! See ya. . PS: I have another dream of building a deep learning library using rust, I really wanna learn about it. That‚Äôs for another post ! . Thank you for reading ! .",
            "url": "abhishekswain.me/roadmap/books/2020/07/09/roadmap.html",
            "relUrl": "/roadmap/books/2020/07/09/roadmap.html",
            "date": " ‚Ä¢ Jul 9, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": ". Hello! I am Abhishek, and this is my blog. Everything I have ever known about the world around me is by listening to life with all my attention. This blog is like an extension of me. I welcome you all to ‚ÄúListening to Life‚Äù . I also post on medium . You can reach me on: linkedin . If you want to have a look at my CV: My CV .",
          "url": "abhishekswain.me/aboutme/",
          "relUrl": "/aboutme/",
          "date": ""
      }
      
  

  

  
      ,"page3": {
          "title": "Projects",
          "content": "MyVision: Customizability meets abstraction . This is my Computer Vision toolkit. It‚Äôs a wrapper over PyTorch combining things I like from different libraries like fastai, keras etc. . Leaf disease detection using image segmentation . This is my final year project about detecting plant leaf disease and segmenting the diseased part. However, there‚Äôs a catch in my project, I have done it without the use of any cnn for segmentation. Instead, it uses a cascade of classifiers to do the detection and segmentation. . CheXNet: Radiologist-Level Pneumonia Detection using deep learning . This project is an implementation of the paper I worked on. CheXNet detects 14 different pathologies from chest X-rays. It uses a Densenet-121 for detection. .",
          "url": "abhishekswain.me/projects/",
          "relUrl": "/projects/",
          "date": ""
      }
      
  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ ‚Äúsitemap.xml‚Äù | absolute_url }} | .",
          "url": "abhishekswain.me/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}