{
  
    
        "post0": {
            "title": "Reflections",
            "content": "Hey there! ğŸ‘‹ğŸ˜ƒ . Finally, after 4 long years I am a graduate. Today I just wanna reflect on 4 years of my B.tech life. Not just that but also about some other things. Itâ€™s gonna be a long one but I just want to write it out here. Lately, I have found writing what I feel to be therapeutic and maybe it will also help me clear my head out. So, here we go. . 2016 was a hard year for me. Nothing seemed to go well. I screwed up all my entrance exams, I could see the disappointment in the eyes of parents. I absolutely had no idea where to go or what to do. I was sad and I was fine with anything they would ask me to do. I joined NIST, which is one of the local engineering college here in Brahmapur. I did not know what to expect. It was very tough for me to comprehend anything, this continuous voice in my head â€œOh you just screwed it up allâ€ kept on going. It was only when my college began was I able to focus on other things. It was the 8th August, 2016 almost exactly 4 years ago. As my first semester classes began, things started getting hectic. We would spend like 12 hrs in college. I would leave by morning 8:30 bus and return home by 7:15 bus in the evening. I became so busy that I could not think of anything. The voice in my head was slowly subsiding. In my first sem I was so hell bent on proving that I could get good marks and stuff that I worked pretty hard. My first sem results came in and I had scored 8.89 sgpa very close to my aim of scoring a 9. My parents brightened up. For the first time in 2 years I saw that I have got so many opportunities lined up for me. Then it dawned on me that it was never about marks, I had to prove a point to myself that I can push through the noise, believe that I can do whatever I wanted to and see that a whole life is up there to live. . That was the last time I cared about my sgpa. Donâ€™t get me wrong but I had always been told that if you acheive a 9-pointer in college, youâ€™ve hit the peak. But as far as I could see, I was far away from what was actually necessary not just in the industry but also to solve problems in general. I could see there were people who made apps, websites and other stuff which I barely had an idea about. That was one of the very first lessons I learnt in my college life. Higher marks do not correlate to higher skills. Infact, your marks only serve as a criterion for eligibility. Thatâ€™s all they are for. Now I am not saying that you should fail your exams, but always remember that they give u eligibility but nothing more than that. Infact slowly the recruitment scenario is moving to skill based, so all these wonâ€™t matter anymore in the near future. This is something I am very happy about. Anyways, letâ€™s move on. The next thing I decided to do was explore the different technologies. This was my first encounter with online courses. Till then I didnâ€™t even know that you could learn almost anything and everything you want from the internet and that too for FREE!. This was the start of a love affair with learining new technologies and making projects out of them which continues till this day. This is when I realised what I love the most, itâ€™s about exploring new frontiers of learning and utilizing them to make things which can contribute to the change or just do it for the fun of it! . â€œThe best thing about software I realized was anything I imagined, I could make into a reality.â€ . In my second year, I took workshops in my college and explored AWS, GCP. I also learnt about web development, it was also a part of my software engineering course in my 4th semester. Now, something real nice happened that year. I happened to come across the Google I/O video on youtube. In that video, Sundar Pichai talked about something called AI and about how they were using it to make their products smarter. It interested me a lot. The fact that machines could be trained to think and reason like humans was fascinating to me. The other reason was that I am a lover of maths, but when youâ€™re taught maths at your school or college youâ€™re never really sure how and why they are applicable to the real world. So, AI was the first time I really saw maths in action, ofcourse to see that took a lot of effort. But, it was really fascinating how seamlessly the different branches of maths integrated with each other and resulted in something which is very similar to what we as humans are capable of doing. I decided to explore this and as I entered the world of AI it just kept fascinating me, to a point now that I live and breathe it. We didnâ€™t have much of a community in my college, there were little to none people doing any work in ML/DL. However I was a determined. I picked up a book from my library. The book was Artificial Intelligence: A modern approach. I really couldnâ€™t make a thing ğŸ˜…. Also I was like where are all the cool applications? There were many algorithms like min-max, a* path finding, etc but I was looking for face recognition, speech to text, etc. For the next few days I did thorough research on the internet. Made a plan. And I was off. Took me sometime to realize that itâ€™s is not something you just do it in few days. Itâ€™s a long process and you keep adapting and learning. And that learning continues to this day. . End of third year I went for my internship at IIT Bombay. That was a turning point in my life in terms in a lot of ways. Learned about working, travelling alone in an unknown city. Most importantly, getting to do what I love to do and also realizing I dont know shit. . The more you know, you realize you know nothing . Final year came with placement preparations, projects, presentations, seminars and so much more. I got placed in TCS which I will be joining this month. With the pandemic just keeping on growing, our joining and final sem exams got shifted by a lot. Like 5 months! . But allâ€™s well that ends well. And here I am finally graduated. Itâ€™s been nice 4 years. I wonâ€™t say fantatsic or anything but I have learnt so much (from the internet and not from college), built few relationships which are here to last. Failed, crashed multiple times but still managed to somehow piece myself together and keep going and I think the bottom line is . Going all out for anything I wanted to do, especially learn and build projects &amp; doing what I love to do . Lately I have been suffering from this problem of comparing myself with others in my field and this feeling of not doing enough. I sometimes set very high expectations from myself and when I am unable to live up to it, that causes sadness. I have tried to analyze this problem and found few issues. One is the issue of learning things properly. I do not learn things properly the first time I try them. This is something I am trying to correct in the future. Next, is I worry too much about people around me doing great work and not spend enough time polishing my existing skills. What I have finally found out as the solution to the complete issue is that everyone has a different journey. A different path through which they learn things. Mine is also different. Also everyone is at a different stage in life. So, comparing myself with others isnâ€™t really fair. Also, I have felt that when I really do what I want to, it is so exciting and commands so much atetntion that I cannot think of anything else. . Excitement is the cause, excitement is the effect . This is taken from the below speech by Sushant Singh Rajput. I decided to add it here since I relate to it so so much. Especially the part about passion and excitement he talks about. Check it out: . I have decided to focus on the present and not much on what others do or the outcome of things. If I am able to keep on improving my self, pushing my boundaries and mostly doing what I love to do with all my attention then I guess this issue will pass. . As always, thank you for reading ğŸ˜ŠğŸ˜ƒ! .",
            "url": "https://abhishekswain.me/reflections/2020/08/12/reflections.html",
            "relUrl": "/reflections/2020/08/12/reflections.html",
            "date": " â€¢ Aug 12, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Logistic regression with a neural network mindset",
            "content": "Hey! This post is about my introduction to the world of Julia. I took this challenge of learning Julia and making something in it. Since Julia is pretty similar to Python, I made a hypothesis. That is can I learn julia and be up and running with something in two days? What I realised is, if you&#39;re from a python background and have some expereince in it, then learning Julia is going to be fun and breezy for you. So, here I am after my two day rendezvous with Julia. . So, what I used to learn Julia? I used resources from julia academy . What did I implement? I decided to go for one the resources I learnt deep learning from: Neural Networks and Deep Learning . Impemented the Julia version of Week 2 assignment of Neural Networks and Deep Learning course. . I hope it&#39;s useful to you. It was a lot of fun and I am in love with Julia â¤ . Let&#39;s begin! . using Random using Plots using HDF5 using Statistics using Base.Iterators . Load dataset . There are two files: train_catvnoncat.h5 &amp; test_catvnoncat.h5 | According to our notation, X is of shape (num_features, num_examples) &amp; y is a row vector of shape (1, num_examples). | We write a function load_dataset() which: Takes in HDF5 files | Converts them into Array{Float64, 2} arrays. | Reshapes them according to our notation &amp; returns X_train, y_train, X_test, y_test | . | function load_dataset(train_file::String, test_file::String) X_train = convert(Array{Float64, 4}, h5read(train_file, &quot;train_set_x&quot;)) y_train = convert(Array{Float64, 1}, h5read(train_file, &quot;train_set_y&quot;)) X_test = convert(Array{Float64, 4}, h5read(test_file, &quot;test_set_x&quot;)) y_test = convert(Array{Float64, 1}, h5read(test_file, &quot;test_set_y&quot;)) num_features_train_X = size(X_train, 1) * size(X_train, 2) * size(X_train, 3) num_features_test_X = size(X_test, 1) * size(X_test, 2) * size(X_test, 3) X_train = reshape(X_train, (num_features_train_X, size(X_train, 4))) y_train = reshape(y_train, (1, size(y_train, 1))) X_test = reshape(X_test, (num_features_test_X, size(X_test, 4))) y_test = reshape(y_test, (1, size(y_test, 1))) X_train, y_train, X_test, y_test end . load_dataset (generic function with 1 method) . X_train, y_train, X_test, y_test = load_dataset(&quot;train_catvnoncat.h5&quot;, &quot;test_catvnoncat.h5&quot;); . @time size(X_train), size(y_train), size(X_test), size(y_test) . 0.002312 seconds (29 allocations: 2.078 KiB) . ((12288, 209), (1, 209), (12288, 50), (1, 50)) . Normalization . X_train, X_test= X_train/255, X_test/255; . @time size(X_train), size(y_train), size(X_test), size(y_test) . 0.000012 seconds (5 allocations: 208 bytes) . ((12288, 209), (1, 209), (12288, 50), (1, 50)) . Mathematical expression of the algorithm: . For one example $x^{(i)}$: $$z^{(i)} = w^T x^{(i)} + b tag{1}$$ $$ hat{y}^{(i)} = a^{(i)} = sigmoid(z^{(i)}) tag{2}$$ $$ mathcal{L}(a^{(i)}, y^{(i)}) = - y^{(i)} log(a^{(i)}) - (1-y^{(i)} ) log(1-a^{(i)}) tag{3}$$ . The cost is then computed by summing over all training examples: $$ J = frac{1}{m} sum_{i=1}^m mathcal{L}(a^{(i)}, y^{(i)}) tag{6}$$ . Sigmoid . Applies sigmoid to the vector . function Ïƒ(z) &quot;&quot;&quot; Compute the sigmoid of z &quot;&quot;&quot; return one(z) / (one(z) + exp(-z)) end . Ïƒ (generic function with 1 method) . Zero initialization . Initialize w &amp; b with with Zeros . function initialize(dim) &quot;&quot;&quot; This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0. Argument: dim -- size of the w vector we want (or number of parameters in this case) Returns: w -- initialized vector of shape (dim, 1) b -- initialized scalar (corresponds to the bias) &quot;&quot;&quot; w = zeros(dim, 1) b = 2 @assert(size(w) == (dim, 1)) @assert(isa(b, Float64) || isa(b, Int64)) return w, b end . initialize (generic function with 1 method) . Forward and Backward propagation . propagate function is the function at the heart of the algorithm. This does the forward prop -&gt; calculate cost -&gt; back-prop. . Forward Propagation: . You get X | You compute $A = sigma(w^T X + b) = (a^{(1)}, a^{(2)}, ..., a^{(m-1)}, a^{(m)})$ | You calculate the cost function: $J = - frac{1}{m} sum_{i=1}^{m}y^{(i)} log(a^{(i)})+(1-y^{(i)}) log(1-a^{(i)})$ | . Here are the two formulas you will be using: . $$ frac{ partial J}{ partial w} = frac{1}{m}X(A-Y)^T tag{7}$$ $$ frac{ partial J}{ partial b} = frac{1}{m} sum_{i=1}^m (a^{(i)}-y^{(i)}) tag{8}$$ . Here is something I love about Julia. It&#39;s that you can directly use symbols as variables ğŸ˜. Doesn&#39;t it look awesome? . function propagate(w, b, X, Y) &quot;&quot;&quot; Implement the cost function and its gradient for the propagation explained above Arguments: w -- weights, a numpy array of size (num_px * num_px * 3, 1) b -- bias, a scalar X -- data of size (num_px * num_px * 3, number of examples) Y -- true &quot;label&quot; vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples) Return: cost -- negative log-likelihood cost for logistic regression dw -- gradient of the loss with respect to w, thus same shape as w db -- gradient of the loss with respect to b, thus same shape as b Tips: - Write your code step by step for the propagation &quot;&quot;&quot; m = size(X, 2) # Forward prop Z = w&#39;X .+ b yÌ‚ = Ïƒ.(Z) @assert(size(yÌ‚) == size(Y)) # Compute cost ğ’¥ = -1 * sum(Y .* log.(yÌ‚) .+ (1 .- Y) .* log.(1 .- yÌ‚)) ğ’¥ /= m @assert(size(ğ’¥) == ()) # Back-prop ğœ•ğ‘§ = yÌ‚ - Y @assert(size(ğœ•ğ‘§) == size(yÌ‚) &amp;&amp; size(ğœ•ğ‘§) == size(Y)) ğœ•ğ‘¤ = (1/m) * X * ğœ•ğ‘§&#39; ğœ•ğ‘ = (1/m) * sum(ğœ•ğ‘§) ğ’¥, Dict(&quot;ğœ•ğ‘¤&quot; =&gt; ğœ•ğ‘¤, &quot;ğœ•ğ‘&quot; =&gt; ğœ•ğ‘) end . propagate (generic function with 1 method) . function optimize(w, b, X, Y, num_iterations, ğ›¼, print_cost) &quot;&quot;&quot; This function optimizes w and b by running a gradient descent algorithm Arguments: w -- weights, a numpy array of size (num_px * num_px * 3, 1) b -- bias, a scalar X -- data of shape (num_px * num_px * 3, number of examples) Y -- true &quot;label&quot; vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples) num_iterations -- number of iterations of the optimization loop learning_rate -- learning rate of the gradient descent update rule print_cost -- True to print the loss every 100 steps Returns: params -- dictionary containing the weights w and bias b grads -- dictionary containing the gradients of the weights and bias with respect to the cost function costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve. Tips: You basically need to write down two steps and iterate through them: 1) Calculate the cost and the gradient for the current parameters. Use propagate(). 2) Update the parameters using gradient descent rule for w and b &quot;&quot;&quot; costs = Array{Float64, 2}(undef, num_iterations, 1) for i=1:num_iterations ğ’¥, ğ›» = propagate(w, b, X, Y) ğœ•ğ‘¤, ğœ•ğ‘ = ğ›»[&quot;ğœ•ğ‘¤&quot;], ğ›»[&quot;ğœ•ğ‘&quot;] global ğœ•ğ‘¤, ğœ•ğ‘ w -= ğ›¼ .* ğœ•ğ‘¤ b -= ğ›¼ .* ğœ•ğ‘ costs[i] = ğ’¥ if print_cost &amp;&amp; i % 100 == 0 println(&quot;Cost after iteration $i = $ğ’¥&quot;) end end params = Dict(&quot;w&quot; =&gt; w, &quot;b&quot; =&gt; b) grads = Dict(&quot;ğœ•ğ‘¤&quot; =&gt; ğœ•ğ‘¤, &quot;ğœ•ğ‘&quot; =&gt; ğœ•ğ‘) params, grads, costs end . optimize (generic function with 1 method) . function predict(w, b, X) &quot;&quot;&quot; Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b) Arguments: w -- weights, a numpy array of size (num_px * num_px * 3, 1) b -- bias, a scalar X -- data of size (num_px * num_px * 3, number of examples) Returns: Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X &quot;&quot;&quot; m = size(X, 2) preds = zeros(1, m) yÌ‚ = Ïƒ.(w&#39;X .+ b) preds = [p &gt; 0.5 ? 1 : 0 for p in Iterators.flatten(yÌ‚)] preds = reshape(preds, (1, m)) @assert(size(preds) == (1, m)) preds end . predict (generic function with 1 method) . Model . Combine all functions to train the model Learning rate: $ alpha = 0.005$, iterations(epochs): 2000 . function model(X_train, y_train, X_test, y_test, num_iterations, ğ›¼, print_cost) &quot;&quot;&quot; Builds the logistic regression model by calling the function you&#39;ve implemented previously Arguments: X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train) Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train) X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test) Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test) num_iterations -- hyperparameter representing the number of iterations to optimize the parameters learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize() print_cost -- Set to true to print the cost every 100 iterations Returns: d -- dictionary containing information about the model. &quot;&quot;&quot; # Initialize parameters w, b = initialize(size(X_train, 1)) # Gradient descent params, grads, costs = optimize(w, b, X_train, y_train, num_iterations, ğ›¼, print_cost) w, b = params[&quot;w&quot;], params[&quot;b&quot;] preds_test = predict(w, b, X_test) preds_train = predict(w, b, X_train) train_acc = 100 - mean(abs.(preds_train - y_train)) * 100 test_acc = 100 - mean(abs.(preds_test - y_test)) * 100 @show train_acc @show test_acc d = Dict( &quot;costs&quot; =&gt; costs, &quot;test_preds&quot; =&gt; preds_test, &quot;train_preds&quot; =&gt; preds_train, &quot;w&quot; =&gt; w, &quot;b&quot; =&gt; b, &quot;ğ›¼&quot; =&gt; ğ›¼, &quot;num_iterations&quot; =&gt; num_iterations ) d; end . model (generic function with 1 method) . d = model(X_train, y_train, X_test, y_test, 2000, 0.005, true); . Cost after iteration 100 = 0.6059553608803026 Cost after iteration 200 = 0.46599325289797994 Cost after iteration 300 = 0.38370595981990246 Cost after iteration 400 = 0.34315419442154194 Cost after iteration 500 = 0.31158754106420994 Cost after iteration 600 = 0.285876628601981 Cost after iteration 700 = 0.26440388180712293 Cost after iteration 800 = 0.24612175591755672 Cost after iteration 900 = 0.23031699193109054 Cost after iteration 1000 = 0.21648420924669995 Cost after iteration 1100 = 0.20425345186200097 Cost after iteration 1200 = 0.1933464382717651 Cost after iteration 1300 = 0.1835489826274778 Cost after iteration 1400 = 0.17469296430212788 Cost after iteration 1500 = 0.16664416505116358 Cost after iteration 1600 = 0.15929383882436224 Cost after iteration 1700 = 0.15255272889996435 Cost after iteration 1800 = 0.1463467326141669 Cost after iteration 1900 = 0.14061370137325302 Cost after iteration 2000 = 0.1353010391796091 train_acc = 99.04306220095694 test_acc = 70.0 . Plotting . x = 1:2000; y = d[&quot;costs&quot;]; gr() # backend plot(x, y, title = &quot;Learning rate = 0.005&quot;, label=&quot;negative log-likelihood&quot;) xlabel!(&quot;iteration&quot;) ylabel!(&quot;cost&quot;) . Epilouge . Okay, you see that the model is clearly overfitting the training data ğŸ˜‚ğŸ¤£. Training accuracy is close to 100%. This is a good sanity check: our model is working and has high enough capacity to fit the training data. Test error is 70%. It is actually not bad for this simple model, given the small dataset we used and that logistic regression is a linear classifier. Further we can reduce overfitting by using regularization etc. . Github repo for notebook: Julia_ML . As always, thank you for reading ğŸ˜ŠğŸ˜ƒ! .",
            "url": "https://abhishekswain.me/machine%20learning/maths/2020/07/28/Logistic_regression-Copy1.html",
            "relUrl": "/machine%20learning/maths/2020/07/28/Logistic_regression-Copy1.html",
            "date": " â€¢ Jul 28, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Statistics and Linear Algebra for ML/DL",
            "content": "Hey there! ğŸ‘‹ğŸ˜ƒ . Today, I am gonna talk about something I have been meaning to do for the longest amout of time. I am now many months into my ML journey, so I have now quite a bit of experience. So, I have a basic understanding of the ML/DL algos but it always feels like I lack in explaining why something works over the others. This is where the mathematical part comes in. Having a sound understanding of the statistical methods is necessary when the explainability comes in. Inorder to do that I have decided to start a Probability &amp; Statistics and Linear Algebra series. This way I would not only be able to learn both but also share with the community which gave me so much. Also, itâ€™s a part of the â€œFeynmann techniqueâ€ which I am so fond of. . Probability &amp; Statistics . Books: . Business statistics for contemporary decision making | Mathematical Statistics with applications | All of statistics | . Video lectures: . Harvard Stats 110 | Professor Leonardâ€™s lectures | Larry Wassermanâ€™s lectures | . Linear Algebra . Books: . Introduction to Linear Algebra (Gilbert Strang) | . Video lectures: . Gilbert Strang lectures | . Applied resources . My favorite: An Introduction to Statistical Learning: With Applications in R | Practical Statistics for Data Scientists: 50+ Essential Concepts Using R and Python | Think Stats(Learn stats using Python) | Introduction to Linear Algebra for Applied Machine Learning with Python | Peter Norvigâ€™s: A Concrete Introduction to Probability (using Python) | . As I read the material and understand a concept I will write an article about it. It will be two parellel series one for each. If you want to follow it then all of it will be posted on my Medium. I will see you there! . As always, thank you for reading ğŸ˜ŠğŸ˜ƒ! . Update(21/07/2020) . I need to strike a balance between theory and application. I actually read few chapters of the book but found that I am more into the code-first approach to learning. So, thereâ€™s a slight modification to my plan. For linear algebra, I am using Computational linear algebra by Rachel Thomas. I have just begun and I already like it. I will use it parellely with Glibert Strangâ€™s linear algebra course. . For proabability &amp; stats: Think Stats + any theory course mentioned. .",
            "url": "https://abhishekswain.me/machine%20learning/maths/books/2020/07/19/statsandlinalg.html",
            "relUrl": "/machine%20learning/maths/books/2020/07/19/statsandlinalg.html",
            "date": " â€¢ Jul 19, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Introduction to Machine Learning course (Refreshing the maths)",
            "content": "Hey there! ğŸ‘‹ğŸ˜ƒ . Today I want to talk about revisiting Machine learning especailly with a focus on underlying mathematics. Well let me tell you, I have just found the perfect course for it. Itâ€™s the Introduction to Machine Learning course. Itâ€™s taught by Prof Balaraman Ravindran. He is someone from whom I have really learnt a lot. And infact the course is by IIT Madras, itâ€™s pretty close to my heart because my introduction to machine learning class almost a year ago was by IIT Madras, not this one but this one ğŸ¤—. I always recommend it to people. Something Iâ€™m really in love with these days is fastai ğŸ¤—ğŸ¤—. I am really in love with Jeremyâ€™s top-down method of teaching. The best thing abouut fastai is you build the whole of PyTorch from ground up. It just doesnâ€™t teach you Deep Learning, but also makes you a better Python programmer. I recommend it to everyone. Fastai also has ML and Computational linear algebra courses. I havenâ€™t gone through them but I bet they are as awesome as the Deep learning one. . Anyways, I have enrolled myself into the NPTEL ML course, so that it can be a refresher. I have my semesters coming up and it starts from the day my sems begin that is 20 &amp; also the whole TCS onboarding to follow but, come on when did we go by the convention? Ofcourse amid all the chaos I am gonna try, thatâ€™s how we roll. Letâ€™s see how it turns out! As always anything and evrything I do I will update it here. Also, I was really confused about what to put on Medium and what here?. Needed to draw a line. I have decided that all tutorials, build &amp; learning journeys go there. Everything except that goes here! Let the sems just get over &amp; the fun will begin. . I want to explore so many things and learn so much. Nothing makes me happier than sharing it with the community that I learnt from. ğŸ¤— And teaching is the best way I have found to effectively learn things. . As always, thank you for reading ğŸ˜ŠğŸ˜ƒ! . (UPDATE 19/07/2020) . . The course was supposed to start from tomorrow but it has got rescheduled to semptember 14. Damn! that is 2 months away. So, need a change of plans. Actually, I had another plan in my mind for the whole maths refresher and getting to know the nitty gritty. The next post will be all about it. . As always, thank you for reading ğŸ˜ŠğŸ˜ƒ! .",
            "url": "https://abhishekswain.me/machine%20learning/2020/07/14/nptelML.html",
            "relUrl": "/machine%20learning/2020/07/14/nptelML.html",
            "date": " â€¢ Jul 14, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Rust v/s C++ for Deep learning",
            "content": "Hey there! ğŸ‘‹ Today, letâ€™s discuss about Rust v/s C++. Actually, itâ€™s not â€œRust v/s C++â€ as in comparison of the languages, but I just want a language for understanding the underpinnings of deep learning algorithms. I want to build them from scratch or atleast try, so that I have a better understanding. One of my favorite physicists, Richard Feynman said something I resonate with a lot: . â€œWhat I cannot create, I do not understand.â€ . So, my search for a language begun. Let me tell you this, I really love Python. Itâ€™s one language I really am very comfortable with and Python was what I used to get into data science (not that there were many others). Python was simple and easy. However, I wanted to do some low level programming. Enter C++. C was my first language way back in school, then C++ &amp; Java. However, it has been years since I actually did something worthwhile with them. I did use Java in my undergrad but it wasnâ€™t as much. Then came Python which I learned as I wanted to get into DS, ML. And I fell in love!. After working with python for many months, one day I discovered that Python is not what is used in deployment! I was like what? really? So, then what? . I found C++ was what most production systems used. In my CS undergrad we had a subject called Systems programming in which we wrote assembly code on IBM 360. I really liked it, it was a pain in the ass to do things as simple as addition, substraction but it really made me understand how the code gets compiled and the internals of it. This started a love with low level programming, I always wanted to do build things from scratch but never got around to do it. It is something I wish I had done! I guess I never thought I could actually do it. But, life is too short for it. Now is the time I finally think of doing it. . Okay, all that aside, I now have narrowed down my choices to Rust &amp; C++. There are other like Swift and Julia. Maybe Julia is more suited to this. But as I said I want to do it with a low level programming language. It would be shit-hard, but I want to challenge myself and if I can do atleast half of it, this would increase my confidence ten-fold. Sundar Pichai in one of his interview said, . â€œTry solving the hardest problem first, if you can then the easier ones will followâ€. . I did extensive research and found: If I use C++, thereâ€™s a job market out there where I can transfer my C++ skills. On the other hand Rust is a pretty young language. But the thing is I gravitate more to Rust &amp; the heart wants what it wants. Naturally, I went through all the blogs and reddit. There were two groups of people. The first kind termed rust as a revolution, as something they called a memory safe language and gave C++ a lot of flak. The second ones said itâ€™s never gonna replace C &amp; C++ anytime soon, pretty correct actually since a lot of code bases is all C &amp; C++. But, code safety with what I was able to understand is basically just bad code. Rust provides a stricter compliler check like you canâ€™t simple pass values around. So what do I do? . I will take my chance &amp; go with Rust â¤ . As always, thank you for reading ğŸ˜ŠğŸ˜ƒ! . (UPDATE) . Wasnâ€™t as simple as I thought ğŸ˜… . This is an update on the Rust v/s C++ post. Itâ€™s been hours I have been hacking through, reading everything I can find about Rust in ML/DL. It wonâ€™t be hard to say that most of them are abandoned. Some are still on but it seems like, thereâ€™s no way they are comaprable to C++ alternatives. Without much of a community around, itâ€™s been pretty hard to get through anything or maybe I havenâ€™t searched enough. I also had a discussion with my friends who are into ML &amp; they suggested me to go for C++. Letâ€™s come to what are the current examples in Rust. The best one I found was huggingface tokenizers. There were also crates like tch-rs which provide Rust bindings for C++ torch api. . So, what is the state of rust in ML/DL? Itâ€™s progressing slowly and steadily, but way further from maturity. Itâ€™s very young and I guess someday we will be able to use Rust for ML/DL, but as of now I think am going to give it a pass. I know it feels like giving up, trust me even I felt so, but the fact is I will be working a full time job as a SDE at TCS soon. With a full time job, I need to be more cautious with how I use my time or spend my energies. While itâ€™s a lovely idea to explore my fantasies but I guess I should be productive at the same time. Also, I will be working with java alot at TCS so that would be like one more langugae added to the mix. In the end, thereâ€™s no effective learning. Lately, by thinking through things I have learnt that I tend to go for new things and fancy stuff. But actually thatâ€™s not whatâ€™s important. Whatever you do, you need rock solid fundamentals. Tooling is trivial as long as you really know what youâ€™re doing. So, I will be giving Rust a pass for now. But definitely one day I will come back to visit rust once again. Till then rather than learning a new language, I will stick what I have atleast some experience with, .i.e. Python, C++ or Java . As always, thank you for reading ğŸ˜ŠğŸ˜ƒ! .",
            "url": "https://abhishekswain.me/deep%20learning/machine%20learning/programming%20language/2020/07/12/rust-vs-cpp.html",
            "relUrl": "/deep%20learning/machine%20learning/programming%20language/2020/07/12/rust-vs-cpp.html",
            "date": " â€¢ Jul 12, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Importance of a proper posture",
            "content": "Today I want to talk about something I had a really bad time with. You must have already guessed from the title, I am going to talk to you about why itâ€™s important to maintain a good posture while doing whatever is that you do, but I will talk to you mostly from programmer point of view. . If you are like me who loves to learn stuff on your own all the time, especailly things like coding, &amp; mostly CS related stuff you must be spending a lot of time infront of you laptop or PC. Now and then I have seen I struggle a lot with a lot of neck pain. So, finally I decided to do a little bit of research. I found out that if my laptop was on level with my eyes or a little above, then I could easily sit for a long time. On the other hand if it was below my eye level and I tried to hunch over and do stuff 7 out of 10 times I would have a stiff neck. And as I am writing this, I do have stiffness in my neck. Itâ€™s very painful. And I donâ€™t want anyone else to suffer from this. So, let me tell you what I have been doing, itâ€™s not much but I hope it will be of some use: . Watch this amazing video by a physiotherapist: | It did me a world of good especially the last exercise. . Apply pain relieving spray or apply balm, I applied omnigel | There are also really good articles explaining why it happens and how you can remedy it. Like this one: Neck Pain: Possible Causes and How to Treat It &amp; The Importance of Posture. . I would also like to tell you what one of my favorite school principals sister Innocentia told me, â€œNever take a chance with your health, if youâ€™re healthy then things can be done at a later time. But if you cripple yourself today maybe you wonâ€™t be able to do it ever againâ€. . Thank you for reading ! .",
            "url": "https://abhishekswain.me/health/lifestyle/2020/07/10/importance-of-posture.html",
            "relUrl": "/health/lifestyle/2020/07/10/importance-of-posture.html",
            "date": " â€¢ Jul 10, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "questus ordo in rebus",
            "content": "This is my first post here and I wanted it to be something I have wanted to do for the longest time. questus ordo in rebus is latin for getting things in order. I was very naive to think that it was pretty simple to do and maybe it is for many people but guess I belong to the latter category where you gotta struggle to do it. Well the bottom line is, I want to get things in order. Itâ€™s not like I have not tried to do it before but there was just so much happening around me, I mean from classes, seminar, minor project to placements, major presentation etc that I couldnâ€™t pause to think about where I am in life and what I want to do moving forward. So, this lockdown period for me was like brakes applied to a car moving at a great speed, itâ€™s kind of pretty sudden and shakes you up but if you think about it maybe it was required?. I think that was the worst analogy I could come up with, but I really am not very good with words. Basically, this lockdown got me pausing and thinking. I wont lie, it was rather frustrating in the beginning because I was used to go walking, running &amp; stuff I did because I wanted to be a â€œfit ğŸ’ª guyâ€ but now that it has been quite sometime I feel it has done me a lot of good. Like I said maybe the brakes were required?. So, letâ€™s begin ! . I feel like things I have learnt I know them pretty well but they seem to be in scatters. Well, not anymore, that ends today. Something which I love to do is read &amp; read a lot. My school notebooks had this thing writetn on the back cover: . â€œReading maketh a full man.â€ . I used to think it was someone in my school who wrote it, but it actually is a quote by Francis Bacon. It resonates with me alot. Reading actually fills your mind with a variety, so the next time you think about anything you look at it from a lot perspectives. And that is where we are going to start today. I have a lot of machine learning, deep learning, statistics, data science books, now this is very cool actually but it also is very overwhelming. Like I get really confused as to where to start. I want to read everything and I try to do it all at once and I end up not reading anything. After screwing up numerous times I have a plan of what I want to do. I have decided I will read one applied book along with a more theoretical book. . The options: . Applied machine learning &amp; deep learning: Approaching almost any machine learning problem | Hands-on machine learning with scikit-learn &amp; tensorflow | Python machine learning by sebestian raschka | Programming collective intelligence | . | Theoretical books: PRML | BRML | ESLR | ISLR(has code examples in R) | Professor Michael I. Jordanâ€™s reading list | &amp; list continuesâ€¦â€¦â€¦â€¦.. | . | . So, what are my choices? . After a lot of research I have decided to go for Hands-on ML &amp; for theory I chose ESLR. I will give my best to it and try and keep posting here about my progress. . Wait a minute! Arenâ€™t we missing something? The younger me would have stopped right here but, I now am a little sensible and I realise no amount of book reading will do me any good if I donâ€™t code and build things up. Enter Kaggle. Besides reading stuff I will try and spend my time implementing things in kaggle! Letâ€™s see how it goes! See ya. . PS: I have another dream of building a deep learning library using rust, I really wanna learn about it. Thatâ€™s for another post ! . Thank you for reading ! .",
            "url": "https://abhishekswain.me/roadmap/books/2020/07/09/roadmap.html",
            "relUrl": "/roadmap/books/2020/07/09/roadmap.html",
            "date": " â€¢ Jul 9, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": ". Hello! I am Abhishek, and this is my blog. Everything I have ever known about the world around me is by listening to life with all my attention. This blog is like an extension of me. I welcome you all to â€œListening to Lifeâ€ . I also post on medium . You can reach me on: linkedin . If you want to have a look at my CV: My CV .",
          "url": "https://abhishekswain.me/aboutme/",
          "relUrl": "/aboutme/",
          "date": ""
      }
      
  

  

  
      ,"page3": {
          "title": "Projects",
          "content": "MyVision: Customizability meets abstraction . This is my Computer Vision toolkit. Itâ€™s a wrapper over PyTorch combining things I like from different libraries like fastai, keras etc. . Leaf disease detection using image segmentation . This is my final year project about detecting plant leaf disease and segmenting the diseased part. However, thereâ€™s a catch in my project, I have done it without the use of any cnn for segmentation. Instead, it uses a cascade of classifiers to do the detection and segmentation. . CheXNet: Radiologist-Level Pneumonia Detection using deep learning . This project is an implementation of the paper I worked on. CheXNet detects 14 different pathologies from chest X-rays. It uses a Densenet-121 for detection. .",
          "url": "https://abhishekswain.me/projects/",
          "relUrl": "/projects/",
          "date": ""
      }
      
  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ â€œsitemap.xmlâ€ | absolute_url }} | .",
          "url": "https://abhishekswain.me/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}