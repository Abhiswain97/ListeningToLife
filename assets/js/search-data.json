{
  
    
        "post0": {
            "title": "Reflections",
            "content": "",
            "url": "https://abhishekswain.me/2020/08/12/reflections.html",
            "relUrl": "/2020/08/12/reflections.html",
            "date": " • Aug 12, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Logistic regression with a neural network mindset",
            "content": "Hey! This post is about my introduction to the world of Julia. I took this challenge of learning Julia and making something in it. Since Julia is pretty similar to Python, I made a hypothesis. That is can I learn julia and be up and running with something in two days? What I realised is, if you&#39;re from a python background and have some expereince in it, then learning Julia is going to be fun and breezy for you. So, here I am after my two day rendezvous with Julia. . So, what I used to learn Julia? I used resources from julia academy . What did I implement? I decided to go for one the resources I learnt deep learning from: Neural Networks and Deep Learning . Impemented the Julia version of Week 2 assignment of Neural Networks and Deep Learning course. . I hope it&#39;s useful to you. It was a lot of fun and I am in love with Julia ❤ . Let&#39;s begin! . using Random using Plots using HDF5 using Statistics using Base.Iterators . Load dataset . There are two files: train_catvnoncat.h5 &amp; test_catvnoncat.h5 | According to our notation, X is of shape (num_features, num_examples) &amp; y is a row vector of shape (1, num_examples). | We write a function load_dataset() which: Takes in HDF5 files | Converts them into Array{Float64, 2} arrays. | Reshapes them according to our notation &amp; returns X_train, y_train, X_test, y_test | . | function load_dataset(train_file::String, test_file::String) X_train = convert(Array{Float64, 4}, h5read(train_file, &quot;train_set_x&quot;)) y_train = convert(Array{Float64, 1}, h5read(train_file, &quot;train_set_y&quot;)) X_test = convert(Array{Float64, 4}, h5read(test_file, &quot;test_set_x&quot;)) y_test = convert(Array{Float64, 1}, h5read(test_file, &quot;test_set_y&quot;)) num_features_train_X = size(X_train, 1) * size(X_train, 2) * size(X_train, 3) num_features_test_X = size(X_test, 1) * size(X_test, 2) * size(X_test, 3) X_train = reshape(X_train, (num_features_train_X, size(X_train, 4))) y_train = reshape(y_train, (1, size(y_train, 1))) X_test = reshape(X_test, (num_features_test_X, size(X_test, 4))) y_test = reshape(y_test, (1, size(y_test, 1))) X_train, y_train, X_test, y_test end . load_dataset (generic function with 1 method) . X_train, y_train, X_test, y_test = load_dataset(&quot;train_catvnoncat.h5&quot;, &quot;test_catvnoncat.h5&quot;); . @time size(X_train), size(y_train), size(X_test), size(y_test) . 0.002312 seconds (29 allocations: 2.078 KiB) . ((12288, 209), (1, 209), (12288, 50), (1, 50)) . Normalization . X_train, X_test= X_train/255, X_test/255; . @time size(X_train), size(y_train), size(X_test), size(y_test) . 0.000012 seconds (5 allocations: 208 bytes) . ((12288, 209), (1, 209), (12288, 50), (1, 50)) . Mathematical expression of the algorithm: . For one example $x^{(i)}$: $$z^{(i)} = w^T x^{(i)} + b tag{1}$$ $$ hat{y}^{(i)} = a^{(i)} = sigmoid(z^{(i)}) tag{2}$$ $$ mathcal{L}(a^{(i)}, y^{(i)}) = - y^{(i)} log(a^{(i)}) - (1-y^{(i)} ) log(1-a^{(i)}) tag{3}$$ . The cost is then computed by summing over all training examples: $$ J = frac{1}{m} sum_{i=1}^m mathcal{L}(a^{(i)}, y^{(i)}) tag{6}$$ . Sigmoid . Applies sigmoid to the vector . function σ(z) &quot;&quot;&quot; Compute the sigmoid of z &quot;&quot;&quot; return one(z) / (one(z) + exp(-z)) end . σ (generic function with 1 method) . Zero initialization . Initialize w &amp; b with with Zeros . function initialize(dim) &quot;&quot;&quot; This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0. Argument: dim -- size of the w vector we want (or number of parameters in this case) Returns: w -- initialized vector of shape (dim, 1) b -- initialized scalar (corresponds to the bias) &quot;&quot;&quot; w = zeros(dim, 1) b = 2 @assert(size(w) == (dim, 1)) @assert(isa(b, Float64) || isa(b, Int64)) return w, b end . initialize (generic function with 1 method) . Forward and Backward propagation . propagate function is the function at the heart of the algorithm. This does the forward prop -&gt; calculate cost -&gt; back-prop. . Forward Propagation: . You get X | You compute $A = sigma(w^T X + b) = (a^{(1)}, a^{(2)}, ..., a^{(m-1)}, a^{(m)})$ | You calculate the cost function: $J = - frac{1}{m} sum_{i=1}^{m}y^{(i)} log(a^{(i)})+(1-y^{(i)}) log(1-a^{(i)})$ | . Here are the two formulas you will be using: . $$ frac{ partial J}{ partial w} = frac{1}{m}X(A-Y)^T tag{7}$$ $$ frac{ partial J}{ partial b} = frac{1}{m} sum_{i=1}^m (a^{(i)}-y^{(i)}) tag{8}$$ . Here is something I love about Julia. It&#39;s that you can directly use symbols as variables 😍. Doesn&#39;t it look awesome? . function propagate(w, b, X, Y) &quot;&quot;&quot; Implement the cost function and its gradient for the propagation explained above Arguments: w -- weights, a numpy array of size (num_px * num_px * 3, 1) b -- bias, a scalar X -- data of size (num_px * num_px * 3, number of examples) Y -- true &quot;label&quot; vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples) Return: cost -- negative log-likelihood cost for logistic regression dw -- gradient of the loss with respect to w, thus same shape as w db -- gradient of the loss with respect to b, thus same shape as b Tips: - Write your code step by step for the propagation &quot;&quot;&quot; m = size(X, 2) # Forward prop Z = w&#39;X .+ b ŷ = σ.(Z) @assert(size(ŷ) == size(Y)) # Compute cost 𝒥 = -1 * sum(Y .* log.(ŷ) .+ (1 .- Y) .* log.(1 .- ŷ)) 𝒥 /= m @assert(size(𝒥) == ()) # Back-prop 𝜕𝑧 = ŷ - Y @assert(size(𝜕𝑧) == size(ŷ) &amp;&amp; size(𝜕𝑧) == size(Y)) 𝜕𝑤 = (1/m) * X * 𝜕𝑧&#39; 𝜕𝑏 = (1/m) * sum(𝜕𝑧) 𝒥, Dict(&quot;𝜕𝑤&quot; =&gt; 𝜕𝑤, &quot;𝜕𝑏&quot; =&gt; 𝜕𝑏) end . propagate (generic function with 1 method) . function optimize(w, b, X, Y, num_iterations, 𝛼, print_cost) &quot;&quot;&quot; This function optimizes w and b by running a gradient descent algorithm Arguments: w -- weights, a numpy array of size (num_px * num_px * 3, 1) b -- bias, a scalar X -- data of shape (num_px * num_px * 3, number of examples) Y -- true &quot;label&quot; vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples) num_iterations -- number of iterations of the optimization loop learning_rate -- learning rate of the gradient descent update rule print_cost -- True to print the loss every 100 steps Returns: params -- dictionary containing the weights w and bias b grads -- dictionary containing the gradients of the weights and bias with respect to the cost function costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve. Tips: You basically need to write down two steps and iterate through them: 1) Calculate the cost and the gradient for the current parameters. Use propagate(). 2) Update the parameters using gradient descent rule for w and b &quot;&quot;&quot; costs = Array{Float64, 2}(undef, num_iterations, 1) for i=1:num_iterations 𝒥, 𝛻 = propagate(w, b, X, Y) 𝜕𝑤, 𝜕𝑏 = 𝛻[&quot;𝜕𝑤&quot;], 𝛻[&quot;𝜕𝑏&quot;] global 𝜕𝑤, 𝜕𝑏 w -= 𝛼 .* 𝜕𝑤 b -= 𝛼 .* 𝜕𝑏 costs[i] = 𝒥 if print_cost &amp;&amp; i % 100 == 0 println(&quot;Cost after iteration $i = $𝒥&quot;) end end params = Dict(&quot;w&quot; =&gt; w, &quot;b&quot; =&gt; b) grads = Dict(&quot;𝜕𝑤&quot; =&gt; 𝜕𝑤, &quot;𝜕𝑏&quot; =&gt; 𝜕𝑏) params, grads, costs end . optimize (generic function with 1 method) . function predict(w, b, X) &quot;&quot;&quot; Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b) Arguments: w -- weights, a numpy array of size (num_px * num_px * 3, 1) b -- bias, a scalar X -- data of size (num_px * num_px * 3, number of examples) Returns: Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X &quot;&quot;&quot; m = size(X, 2) preds = zeros(1, m) ŷ = σ.(w&#39;X .+ b) preds = [p &gt; 0.5 ? 1 : 0 for p in Iterators.flatten(ŷ)] preds = reshape(preds, (1, m)) @assert(size(preds) == (1, m)) preds end . predict (generic function with 1 method) . Model . Combine all functions to train the model Learning rate: $ alpha = 0.005$, iterations(epochs): 2000 . function model(X_train, y_train, X_test, y_test, num_iterations, 𝛼, print_cost) &quot;&quot;&quot; Builds the logistic regression model by calling the function you&#39;ve implemented previously Arguments: X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train) Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train) X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test) Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test) num_iterations -- hyperparameter representing the number of iterations to optimize the parameters learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize() print_cost -- Set to true to print the cost every 100 iterations Returns: d -- dictionary containing information about the model. &quot;&quot;&quot; # Initialize parameters w, b = initialize(size(X_train, 1)) # Gradient descent params, grads, costs = optimize(w, b, X_train, y_train, num_iterations, 𝛼, print_cost) w, b = params[&quot;w&quot;], params[&quot;b&quot;] preds_test = predict(w, b, X_test) preds_train = predict(w, b, X_train) train_acc = 100 - mean(abs.(preds_train - y_train)) * 100 test_acc = 100 - mean(abs.(preds_test - y_test)) * 100 @show train_acc @show test_acc d = Dict( &quot;costs&quot; =&gt; costs, &quot;test_preds&quot; =&gt; preds_test, &quot;train_preds&quot; =&gt; preds_train, &quot;w&quot; =&gt; w, &quot;b&quot; =&gt; b, &quot;𝛼&quot; =&gt; 𝛼, &quot;num_iterations&quot; =&gt; num_iterations ) d; end . model (generic function with 1 method) . d = model(X_train, y_train, X_test, y_test, 2000, 0.005, true); . Cost after iteration 100 = 0.6059553608803026 Cost after iteration 200 = 0.46599325289797994 Cost after iteration 300 = 0.38370595981990246 Cost after iteration 400 = 0.34315419442154194 Cost after iteration 500 = 0.31158754106420994 Cost after iteration 600 = 0.285876628601981 Cost after iteration 700 = 0.26440388180712293 Cost after iteration 800 = 0.24612175591755672 Cost after iteration 900 = 0.23031699193109054 Cost after iteration 1000 = 0.21648420924669995 Cost after iteration 1100 = 0.20425345186200097 Cost after iteration 1200 = 0.1933464382717651 Cost after iteration 1300 = 0.1835489826274778 Cost after iteration 1400 = 0.17469296430212788 Cost after iteration 1500 = 0.16664416505116358 Cost after iteration 1600 = 0.15929383882436224 Cost after iteration 1700 = 0.15255272889996435 Cost after iteration 1800 = 0.1463467326141669 Cost after iteration 1900 = 0.14061370137325302 Cost after iteration 2000 = 0.1353010391796091 train_acc = 99.04306220095694 test_acc = 70.0 . Plotting . x = 1:2000; y = d[&quot;costs&quot;]; gr() # backend plot(x, y, title = &quot;Learning rate = 0.005&quot;, label=&quot;negative log-likelihood&quot;) xlabel!(&quot;iteration&quot;) ylabel!(&quot;cost&quot;) . Epilouge . Okay, you see that the model is clearly overfitting the training data 😂🤣. Training accuracy is close to 100%. This is a good sanity check: our model is working and has high enough capacity to fit the training data. Test error is 70%. It is actually not bad for this simple model, given the small dataset we used and that logistic regression is a linear classifier. Further we can reduce overfitting by using regularization etc. . Github repo for notebook: Julia_ML . As always, thank you for reading 😊😃! .",
            "url": "https://abhishekswain.me/machine%20learning/maths/2020/07/28/Logistic_regression-Copy1.html",
            "relUrl": "/machine%20learning/maths/2020/07/28/Logistic_regression-Copy1.html",
            "date": " • Jul 28, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Statistics and Linear Algebra for ML/DL",
            "content": "Hey there! 👋😃 . Today, I am gonna talk about something I have been meaning to do for the longest amout of time. I am now many months into my ML journey, so I have now quite a bit of experience. So, I have a basic understanding of the ML/DL algos but it always feels like I lack in explaining why something works over the others. This is where the mathematical part comes in. Having a sound understanding of the statistical methods is necessary when the explainability comes in. Inorder to do that I have decided to start a Probability &amp; Statistics and Linear Algebra series. This way I would not only be able to learn both but also share with the community which gave me so much. Also, it’s a part of the “Feynmann technique” which I am so fond of. . Probability &amp; Statistics . Books: . Business statistics for contemporary decision making | Mathematical Statistics with applications | All of statistics | . Video lectures: . Harvard Stats 110 | Professor Leonard’s lectures | Larry Wasserman’s lectures | . Linear Algebra . Books: . Introduction to Linear Algebra (Gilbert Strang) | . Video lectures: . Gilbert Strang lectures | . Applied resources . My favorite: An Introduction to Statistical Learning: With Applications in R | Practical Statistics for Data Scientists: 50+ Essential Concepts Using R and Python | Think Stats(Learn stats using Python) | Introduction to Linear Algebra for Applied Machine Learning with Python | Peter Norvig’s: A Concrete Introduction to Probability (using Python) | . As I read the material and understand a concept I will write an article about it. It will be two parellel series one for each. If you want to follow it then all of it will be posted on my Medium. I will see you there! . As always, thank you for reading 😊😃! . Update(21/07/2020) . I need to strike a balance between theory and application. I actually read few chapters of the book but found that I am more into the code-first approach to learning. So, there’s a slight modification to my plan. For linear algebra, I am using Computational linear algebra by Rachel Thomas. I have just begun and I already like it. I will use it parellely with Glibert Strang’s linear algebra course. . For proabability &amp; stats: Think Stats + any theory course mentioned. .",
            "url": "https://abhishekswain.me/machine%20learning/maths/books/2020/07/19/statsandlinalg.html",
            "relUrl": "/machine%20learning/maths/books/2020/07/19/statsandlinalg.html",
            "date": " • Jul 19, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Introduction to Machine Learning course (Refreshing the maths)",
            "content": "Hey there! 👋😃 . Today I want to talk about revisiting Machine learning especailly with a focus on underlying mathematics. Well let me tell you, I have just found the perfect course for it. It’s the Introduction to Machine Learning course. It’s taught by Prof Balaraman Ravindran. He is someone from whom I have really learnt a lot. And infact the course is by IIT Madras, it’s pretty close to my heart because my introduction to machine learning class almost a year ago was by IIT Madras, not this one but this one 🤗. I always recommend it to people. Something I’m really in love with these days is fastai 🤗🤗. I am really in love with Jeremy’s top-down method of teaching. The best thing abouut fastai is you build the whole of PyTorch from ground up. It just doesn’t teach you Deep Learning, but also makes you a better Python programmer. I recommend it to everyone. Fastai also has ML and Computational linear algebra courses. I haven’t gone through them but I bet they are as awesome as the Deep learning one. . Anyways, I have enrolled myself into the NPTEL ML course, so that it can be a refresher. I have my semesters coming up and it starts from the day my sems begin that is 20 &amp; also the whole TCS onboarding to follow but, come on when did we go by the convention? Ofcourse amid all the chaos I am gonna try, that’s how we roll. Let’s see how it turns out! As always anything and evrything I do I will update it here. Also, I was really confused about what to put on Medium and what here?. Needed to draw a line. I have decided that all tutorials, build &amp; learning journeys go there. Everything except that goes here! Let the sems just get over &amp; the fun will begin. . I want to explore so many things and learn so much. Nothing makes me happier than sharing it with the community that I learnt from. 🤗 And teaching is the best way I have found to effectively learn things. . As always, thank you for reading 😊😃! . (UPDATE 19/07/2020) . . The course was supposed to start from tomorrow but it has got rescheduled to semptember 14. Damn! that is 2 months away. So, need a change of plans. Actually, I had another plan in my mind for the whole maths refresher and getting to know the nitty gritty. The next post will be all about it. . As always, thank you for reading 😊😃! .",
            "url": "https://abhishekswain.me/machine%20learning/2020/07/14/nptelML.html",
            "relUrl": "/machine%20learning/2020/07/14/nptelML.html",
            "date": " • Jul 14, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Rust v/s C++ for Deep learning",
            "content": "Hey there! 👋 Today, let’s discuss about Rust v/s C++. Actually, it’s not “Rust v/s C++” as in comparison of the languages, but I just want a language for understanding the underpinnings of deep learning algorithms. I want to build them from scratch or atleast try, so that I have a better understanding. One of my favorite physicists, Richard Feynman said something I resonate with a lot: . “What I cannot create, I do not understand.” . So, my search for a language begun. Let me tell you this, I really love Python. It’s one language I really am very comfortable with and Python was what I used to get into data science (not that there were many others). Python was simple and easy. However, I wanted to do some low level programming. Enter C++. C was my first language way back in school, then C++ &amp; Java. However, it has been years since I actually did something worthwhile with them. I did use Java in my undergrad but it wasn’t as much. Then came Python which I learned as I wanted to get into DS, ML. And I fell in love!. After working with python for many months, one day I discovered that Python is not what is used in deployment! I was like what? really? So, then what? . I found C++ was what most production systems used. In my CS undergrad we had a subject called Systems programming in which we wrote assembly code on IBM 360. I really liked it, it was a pain in the ass to do things as simple as addition, substraction but it really made me understand how the code gets compiled and the internals of it. This started a love with low level programming, I always wanted to do build things from scratch but never got around to do it. It is something I wish I had done! I guess I never thought I could actually do it. But, life is too short for it. Now is the time I finally think of doing it. . Okay, all that aside, I now have narrowed down my choices to Rust &amp; C++. There are other like Swift and Julia. Maybe Julia is more suited to this. But as I said I want to do it with a low level programming language. It would be shit-hard, but I want to challenge myself and if I can do atleast half of it, this would increase my confidence ten-fold. Sundar Pichai in one of his interview said, . “Try solving the hardest problem first, if you can then the easier ones will follow”. . I did extensive research and found: If I use C++, there’s a job market out there where I can transfer my C++ skills. On the other hand Rust is a pretty young language. But the thing is I gravitate more to Rust &amp; the heart wants what it wants. Naturally, I went through all the blogs and reddit. There were two groups of people. The first kind termed rust as a revolution, as something they called a memory safe language and gave C++ a lot of flak. The second ones said it’s never gonna replace C &amp; C++ anytime soon, pretty correct actually since a lot of code bases is all C &amp; C++. But, code safety with what I was able to understand is basically just bad code. Rust provides a stricter compliler check like you can’t simple pass values around. So what do I do? . I will take my chance &amp; go with Rust ❤ . As always, thank you for reading 😊😃! . (UPDATE) . Wasn’t as simple as I thought 😅 . This is an update on the Rust v/s C++ post. It’s been hours I have been hacking through, reading everything I can find about Rust in ML/DL. It won’t be hard to say that most of them are abandoned. Some are still on but it seems like, there’s no way they are comaprable to C++ alternatives. Without much of a community around, it’s been pretty hard to get through anything or maybe I haven’t searched enough. I also had a discussion with my friends who are into ML &amp; they suggested me to go for C++. Let’s come to what are the current examples in Rust. The best one I found was huggingface tokenizers. There were also crates like tch-rs which provide Rust bindings for C++ torch api. . So, what is the state of rust in ML/DL? It’s progressing slowly and steadily, but way further from maturity. It’s very young and I guess someday we will be able to use Rust for ML/DL, but as of now I think am going to give it a pass. I know it feels like giving up, trust me even I felt so, but the fact is I will be working a full time job as a SDE at TCS soon. With a full time job, I need to be more cautious with how I use my time or spend my energies. While it’s a lovely idea to explore my fantasies but I guess I should be productive at the same time. Also, I will be working with java alot at TCS so that would be like one more langugae added to the mix. In the end, there’s no effective learning. Lately, by thinking through things I have learnt that I tend to go for new things and fancy stuff. But actually that’s not what’s important. Whatever you do, you need rock solid fundamentals. Tooling is trivial as long as you really know what you’re doing. So, I will be giving Rust a pass for now. But definitely one day I will come back to visit rust once again. Till then rather than learning a new language, I will stick what I have atleast some experience with, .i.e. Python, C++ or Java . As always, thank you for reading 😊😃! .",
            "url": "https://abhishekswain.me/deep%20learning/machine%20learning/programming%20language/2020/07/12/rust-vs-cpp.html",
            "relUrl": "/deep%20learning/machine%20learning/programming%20language/2020/07/12/rust-vs-cpp.html",
            "date": " • Jul 12, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Importance of a proper posture",
            "content": "Today I want to talk about something I had a really bad time with. You must have already guessed from the title, I am going to talk to you about why it’s important to maintain a good posture while doing whatever is that you do, but I will talk to you mostly from programmer point of view. . If you are like me who loves to learn stuff on your own all the time, especailly things like coding, &amp; mostly CS related stuff you must be spending a lot of time infront of you laptop or PC. Now and then I have seen I struggle a lot with a lot of neck pain. So, finally I decided to do a little bit of research. I found out that if my laptop was on level with my eyes or a little above, then I could easily sit for a long time. On the other hand if it was below my eye level and I tried to hunch over and do stuff 7 out of 10 times I would have a stiff neck. And as I am writing this, I do have stiffness in my neck. It’s very painful. And I don’t want anyone else to suffer from this. So, let me tell you what I have been doing, it’s not much but I hope it will be of some use: . Watch this amazing video by a physiotherapist: | It did me a world of good especially the last exercise. . Apply pain relieving spray or apply balm, I applied omnigel | There are also really good articles explaining why it happens and how you can remedy it. Like this one: Neck Pain: Possible Causes and How to Treat It &amp; The Importance of Posture. . I would also like to tell you what one of my favorite school principals sister Innocentia told me, “Never take a chance with your health, if you’re healthy then things can be done at a later time. But if you cripple yourself today maybe you won’t be able to do it ever again”. . Thank you for reading ! .",
            "url": "https://abhishekswain.me/health/lifestyle/2020/07/10/importance-of-posture.html",
            "relUrl": "/health/lifestyle/2020/07/10/importance-of-posture.html",
            "date": " • Jul 10, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "questus ordo in rebus",
            "content": "This is my first post here and I wanted it to be something I have wanted to do for the longest time. questus ordo in rebus is latin for getting things in order. I was very naive to think that it was pretty simple to do and maybe it is for many people but guess I belong to the latter category where you gotta struggle to do it. Well the bottom line is, I want to get things in order. It’s not like I have not tried to do it before but there was just so much happening around me, I mean from classes, seminar, minor project to placements, major presentation etc that I couldn’t pause to think about where I am in life and what I want to do moving forward. So, this lockdown period for me was like brakes applied to a car moving at a great speed, it’s kind of pretty sudden and shakes you up but if you think about it maybe it was required?. I think that was the worst analogy I could come up with, but I really am not very good with words. Basically, this lockdown got me pausing and thinking. I wont lie, it was rather frustrating in the beginning because I was used to go walking, running &amp; stuff I did because I wanted to be a “fit 💪 guy” but now that it has been quite sometime I feel it has done me a lot of good. Like I said maybe the brakes were required?. So, let’s begin ! . I feel like things I have learnt I know them pretty well but they seem to be in scatters. Well, not anymore, that ends today. Something which I love to do is read &amp; read a lot. My school notebooks had this thing writetn on the back cover: . “Reading maketh a full man.” . I used to think it was someone in my school who wrote it, but it actually is a quote by Francis Bacon. It resonates with me alot. Reading actually fills your mind with a variety, so the next time you think about anything you look at it from a lot perspectives. And that is where we are going to start today. I have a lot of machine learning, deep learning, statistics, data science books, now this is very cool actually but it also is very overwhelming. Like I get really confused as to where to start. I want to read everything and I try to do it all at once and I end up not reading anything. After screwing up numerous times I have a plan of what I want to do. I have decided I will read one applied book along with a more theoretical book. . The options: . Applied machine learning &amp; deep learning: Approaching almost any machine learning problem | Hands-on machine learning with scikit-learn &amp; tensorflow | Python machine learning by sebestian raschka | Programming collective intelligence | . | Theoretical books: PRML | BRML | ESLR | ISLR(has code examples in R) | Professor Michael I. Jordan’s reading list | &amp; list continues………….. | . | . So, what are my choices? . After a lot of research I have decided to go for Hands-on ML &amp; for theory I chose ESLR. I will give my best to it and try and keep posting here about my progress. . Wait a minute! Aren’t we missing something? The younger me would have stopped right here but, I now am a little sensible and I realise no amount of book reading will do me any good if I don’t code and build things up. Enter Kaggle. Besides reading stuff I will try and spend my time implementing things in kaggle! Let’s see how it goes! See ya. . PS: I have another dream of building a deep learning library using rust, I really wanna learn about it. That’s for another post ! . Thank you for reading ! .",
            "url": "https://abhishekswain.me/roadmap/books/2020/07/09/roadmap.html",
            "relUrl": "/roadmap/books/2020/07/09/roadmap.html",
            "date": " • Jul 9, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": ". Hello! I am Abhishek, and this is my blog. Everything I have ever known about the world around me is by listening to life with all my attention. This blog is like an extension of me. I welcome you all to “Listening to Life” . I also post on medium . You can reach me on: linkedin . If you want to have a look at my CV: My CV .",
          "url": "https://abhishekswain.me/aboutme/",
          "relUrl": "/aboutme/",
          "date": ""
      }
      
  

  

  
      ,"page3": {
          "title": "Projects",
          "content": "MyVision: Customizability meets abstraction . This is my Computer Vision toolkit. It’s a wrapper over PyTorch combining things I like from different libraries like fastai, keras etc. . Leaf disease detection using image segmentation . This is my final year project about detecting plant leaf disease and segmenting the diseased part. However, there’s a catch in my project, I have done it without the use of any cnn for segmentation. Instead, it uses a cascade of classifiers to do the detection and segmentation. . CheXNet: Radiologist-Level Pneumonia Detection using deep learning . This project is an implementation of the paper I worked on. CheXNet detects 14 different pathologies from chest X-rays. It uses a Densenet-121 for detection. .",
          "url": "https://abhishekswain.me/projects/",
          "relUrl": "/projects/",
          "date": ""
      }
      
  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://abhishekswain.me/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}